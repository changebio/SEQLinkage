{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core module\n",
    "\n",
    "> API details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SEQLinkage.Main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.run_linkage=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;40;32mMESSAGE: Binary trait detected in [/mnt/mfs/statgen/yin/Github/linkage/sample_i/rare_positions/sample_i_coding.hg38_multianno.fam]\u001b[0m\n",
      "\u001b[1;40;32mMESSAGE: Checking local resources 5/5 ...\u001b[0m\n",
      "\u001b[1;40;32mMESSAGE: 166 samples found in [/mnt/mfs/statgen/yin/Github/linkage/sample_i/rare_positions/sample_i_coding.hg38_multianno.vcf.gz]\u001b[0m\n",
      "\u001b[1;40;32mMESSAGE: 35 families with a total of 166 samples will be scanned for 28,325 pre-defined units\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, 0, ['1', '11868', '14362', 'LOC102725121@1', '9.177127474362311e-07', '1.1657192989882668e-06', '6.814189157634088e-07'])\n",
      "(1000, 3, 81, ['1', '61742479', '62163915', 'PATJ', '94.08912254193991', '118.69606841169087', '71.63174343660968'])\n",
      "(2000, 3, 7, ['1', '162790701', '162812818', 'HSD17B7', '168.91829636079495', '218.49549236802426', '122.20452659549645'])\n",
      "(3000, 1, 0, ['2', '27212359', '27217182', 'ATRAID', '48.19894998090023', '54.585310839481366', '42.49830744092714'])\n",
      "(4000, 1, 0, ['2', '168920780', '169031324', 'ABCB11', '174.1718395156357', '222.22338807109554', '127.6844708222256'])\n",
      "(5000, 1, 0, ['3', '47413680', '47475941', 'SCAP', '68.99620573382401', '81.61445932132666', '57.9855449134992'])\n",
      "(6000, 1, 0, ['3', '184230352', '184242329', 'VWA5B2', '192.1275334861548', '247.45672160907745', '139.2580545053618'])\n",
      "(7000, 1, 0, ['4', '138164096', '138242349', 'SLC7A11', '140.35374100483878', '186.05001188945482', '97.20783019850205'])\n",
      "(8000, 1, 0, ['5', '132190146', '132227853', 'P4HA2', '138.85770413097532', '173.97439801388742', '105.14936291323124'])\n",
      "(9000, 1, 0, ['6', '31615233', '31617015', 'AIF1', '53.57404706283638', '56.98460061173304', '51.484032621407884'])\n",
      "(10000, 1, 0, ['6', '167155246', '167157980', 'GPR31', '187.8212404552102', '243.3868606324144', '134.9096549771634'])\n",
      "(11000, 1, 0, ['7', '113080412', '113087778', 'GPR85', '121.94724950804803', '163.03027920118186', '82.53468270653605'])\n",
      "(12000, 1, 0, ['8', '81478418', '81483233', 'FABP4', '95.00012872224329', '125.79097683477232', '65.891333925847'])\n",
      "(13000, 1, 0, ['9', '95085207', '95085304', 'MIR23B', '100.77698198363763', '124.30346632003499', '78.55681544180344'])\n",
      "(14000, 1, 0, ['10', '58325628', '58329679', 'LOC112268068', '74.88956742062021', '94.58971593360282', '57.27601225803206'])\n",
      "(15000, 3, 45, ['11', '12286899', '12362138', 'MICALCL', '25.200421811362663', '19.780182273947002', '30.47861020169645'])\n",
      "(16000, 3, 19, ['11', '101890673', '101916522', 'ANGPTL5', '109.59131065684834', '139.3668510208155', '80.95094926518013'])\n",
      "(17000, 3, 27, ['12', '54395260', '54419266', 'ITGA5', '72.35531487721212', '85.30635133411202', '60.57391809500565'])\n",
      "(18000, 1, 0, ['13', '46455203', '46466776', 'LINC01198', '50.167966040012665', '57.175574695883995', '43.125175052271366'])\n",
      "(19000, 1, 0, ['14', '86905777', '86922755', 'LINC01148', '84.32939464542692', '103.04064213211844', '66.78106340164302'])\n",
      "(20000, 3, 12, ['15', '73873563', '73889214', 'TBC1D21', '77.64806530153696', '97.22184379127768', '58.61042722911022'])\n",
      "(21000, 1, 0, ['16', '33495800', '33496235', 'LOC390705@2', '59.597025459459736', '64.51252841288981', '55.757286567026384'])\n",
      "(22000, 1, 0, ['17', '28360653', '28360734', 'MIR4723', '53.29906731104194', '56.137810791384275', '49.8686532838509'])\n",
      "(23000, 3, 3, ['17', '79823451', '79827704', 'LINC01977', '129.41707434188964', '161.0935891249485', '98.15793907670367'])\n",
      "(24000, 3, 16, ['19', '12953118', '12957223', 'GADD45GIP1', '33.20123193440195', '31.021078181726253', '35.714493705610096'])\n",
      "(25000, 3, 23, ['19', '51964339', '51986840', 'ZNF350', '89.6950481101567', '113.9831205106624', '66.44497607852102'])\n",
      "(26000, 1, 0, ['20', '62143768', '62182514', 'SS18L1', '110.30700665717275', '124.48353231287628', '98.56286538747486'])\n",
      "(27000, 1, 0, ['22', '43110749', '43129712', 'BIK', '54.13818873093194', '71.04904715507725', '37.884233842626216'])\n",
      "(28000, 1, 0, ['X', '130494940', '130497447', 'DENND10P1', 'NA', '146.88487435565696', 'NA'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;40;32mMESSAGE: 10,073 units (from 304,169 variants) processed; 0 Mendelian inconsistencies and 0 recombination events handled\u001b[0m\n",
      "\u001b[1;40;32mMESSAGE: 15,549 units ignored due to absence in VCF file\u001b[0m\n",
      "\u001b[1;40;32mMESSAGE: 2,703 units ignored due to absence of variation in samples\u001b[0m\n",
      "\u001b[1;40;32mMESSAGE: Archiving regional marker data to directory [./cache]\u001b[0m\n",
      "\u001b[1;40;32mMESSAGE: 10,073 units will be converted to MERLIN format\u001b[0m\n",
      "\u001b[1;40;32mMESSAGE: 0 units successfully converted to MERLIN format\u001b[0m\n",
      "\u001b[1;40;32mMESSAGE: Archiving MERLIN format to directory [./cache]\u001b[0m\n",
      "\u001b[1;40;32mMESSAGE: Saving data to [/mnt/mfs/statgen/yin/Github/linkage/SEQpy2/testseqlink]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args().parser.parse_args(['--fam', '../seqlinkage-example/seqlinkage-example.fam', '--vcf', '../seqlinkage-example/seqlinkage-example.vcf.gz', '-f', 'MERLIN','--run-linkage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args().parser.parse_args(['--fam','../sample_i/rare_positions/sample_i_coding.hg38_multianno.fam', \n",
    "                                 '--vcf', '../sample_i/rare_positions/sample_i_coding.hg38_multianno.vcf.gz',\n",
    "                                 '--blueprint','./MWE/genemap.hg38.txt', '--chrom-prefix','1','-f','MERLIN',\n",
    "'--tempdir','./Tempdir_s1',\n",
    "'--build', 'hg38',  '--freq', 'AF', '-K', '0.001', '--moi', 'AD', '-W', '0', '-M', '1', \n",
    "'--theta-max', '0.5', '--theta-inc', '0.05','--run-linkage', '--output', './testseqlink'])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args().parser.parse_args(['--fam','../seqlinkage-example/seqlinkage-example.fam','--vcf','../seqlinkage-example/seqlinkage-example.vcf.gz',\n",
    "                                 '--build','hg38','--recomb_cross_fam','--freq','EVSEAAF','-K','0.001','--moi','AR','-W','0','-M','1',\n",
    "                                 '--theta-max','0.5','--theta-inc','0.05','--run-linkage','--output','./testseqlink'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "?shoud we set mle parameter as true? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args().parser.parse_args(['--fam','../MWE/sample2_uniq.fam', \n",
    "                                 '--vcf', '../MWE/sample_ii_coding.hg38_multianno.vcf.gz',\n",
    "'--blueprint','../MWE/genemap.hg38.txt', '--chrom-prefix','1','-f','MERLIN',\n",
    "'--tempdir','./Tempdir',\n",
    "'--build', 'hg38',  '--freq', 'AF', '-K', '0.001', '--moi', 'AD', '-W', '0', '-M', '1', \n",
    "'--theta-max', '0.5', '--theta-inc', '0.05','--run-linkage', '--output', './testseqlink1105'])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(bin=0.8, blueprint='./MWE/genemap.hg38.txt', build='hg38', chr_prefix='1', debug=False, format=['MERLIN'], freq='AF', freq_by_fam=None, func=<function main at 0x2b981a9ffad0>, include_vars=None, inherit_mode='AD', jobs=16, maf_cutoff=1.0, mle=False, muta_pen=1.0, no_save=False, output='./testseqlink', output_limit=10, prephased=False, prevalence=0.001, quiet=False, recomb_cross_fam=False, recomb_max=1, rsq=0.0, run_linkage=True, rvhaplo=False, single_markers=False, tempdir='./Tempdir_s1', tfam='../sample_i/rare_positions/sample_i_coding.hg38_multianno.fam', theta_inc=0.05, theta_max=0.5, vanilla=True, vcf='../sample_i/rare_positions/sample_i_coding.hg38_multianno.vcf.gz', wild_pen=0.0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.from Core import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from __future__ import print_function\n",
    "from SEQLinkage.Utils import *\n",
    "from SEQLinkage.Runner import *\n",
    "from multiprocessing import Process, Queue\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import sys, faulthandler, platform\n",
    "import numpy as np\n",
    "import os\n",
    "if sys.version_info.major == 2:\n",
    "    from cstatgen import cstatgen_py2 as cstatgen\n",
    "    from cstatgen.egglib import Align\n",
    "else:\n",
    "    from cstatgen import cstatgen_py3 as cstatgen\n",
    "    from egglib import Align"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. RData class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RData(dict):\n",
    "    def __init__(self, vcf, tfam):\n",
    "        # tfam.samples: a dict of {sid:[fid, pid, mid, sex, trait], ...}\n",
    "        # tfam.families: a dict of {fid:[s1, s2 ...], ...}\n",
    "        self.tfam = TFAMParser(tfam)\n",
    "        self.vs = self.load_vcf(vcf)\n",
    "        self.samples_vcf = self.vs.GetSampleNames()\n",
    "        self.samples_not_vcf = checkSamples(self.samples_vcf, self.tfam.samples.keys())[1]\n",
    "        # samples have to be in both vcf and tfam data\n",
    "        self.samples = OrderedDict([(k, self.tfam.samples[k]) for k in self.samples_vcf if k in self.tfam.samples])\n",
    "        # a dict of {fid:[member names], ...}\n",
    "        self.families = {k : [x for x in self.samples if x in self.tfam.families[k]] for k in self.tfam.families}\n",
    "        # a dict of {fid:[idx ...], ...}\n",
    "        self.famsampidx = {}\n",
    "        # a dict of {fid:[maf1, maf2 ...]}\n",
    "        self.maf = OrderedDict()\n",
    "        # finalized sub_regions that are compliant to all families\n",
    "        self.complied_markers = []\n",
    "        # finalized sub regions (variants)\n",
    "        self.combined_regions = []\n",
    "        self.coordinates_by_region = []\n",
    "        # RV varnames by family\n",
    "        self.varnames_by_fam = {}\n",
    "        self.patterns={}\n",
    "        self.gnomAD_estimate={'AFR':(1-0.4589)/(2*7652),'AMR':(1-0.4455)/(2*16791),'ASJ':(1-0.2357)/(2*4925),'EAS':(1-0.4735)/(2*8624),'FIN':(1-0.3048)/(2*11150),'NFE':(1-0.5729)/(2*55860),'OTH':(1-0.4386)/(2*2743),'SAS':(1-0.5624)/(2*15391)}\n",
    "        # reorder family samples based on order of VCF file\n",
    "        for k in self.families.keys():\n",
    "            if len(self.families[k]) == 0:\n",
    "                # skip families having no samples in VCF file\n",
    "                del self.families[k]\n",
    "            else:\n",
    "                self.famsampidx[k] = [i for i, x in enumerate(self.samples_vcf) if x in self.families[k]]\n",
    "        # a dict of {fid:[idx ...], ...}\n",
    "        self.famvaridx = {}\n",
    "        self.wtvar = {}\n",
    "        self.freq_by_fam = {}\n",
    "        self.include_vars = []\n",
    "        self.total_varnames={}\n",
    "        self.total_mafs={}\n",
    "        self.wt_maf={}\n",
    "        self.freq = []\n",
    "        self.genotype_all={}\n",
    "        self.mle_mafs={}\n",
    "        self.missing_persons=[]\n",
    "        self.reset()\n",
    "    \n",
    "    def load_vcf(self,vcf):\n",
    "        # load VCF file header\n",
    "        return cstatgen.VCFstream(vcf)\n",
    "\n",
    "    def reset(self):\n",
    "        for item in self.samples:\n",
    "            self[item] = []\n",
    "            self.genotype_all[item] = []\n",
    "        self.variants = []\n",
    "        self.include_vars = []\n",
    "        self.total_varnames={}\n",
    "        self.total_mafs={}\n",
    "        self.wt_maf={}\n",
    "        self.chrom = None\n",
    "        for k in self.families:\n",
    "            self.famvaridx[k] = []\n",
    "            self.wtvar[k] = []\n",
    "        self.maf = OrderedDict()\n",
    "        # superMarkerCount is the max num. of recombinant fragments among all fams\n",
    "        self.superMarkerCount = 0\n",
    "        self.complied_markers = []\n",
    "        self.combined_regions = []\n",
    "        self.coordinates_by_region = []\n",
    "        self.patterns={}\n",
    "        self.missing_persons=[]\n",
    "\n",
    "    def getMidPosition(self):\n",
    "        if len(self.variants) == 0:\n",
    "            return None\n",
    "        return sum([x[1] for x in self.variants]) / len(self.variants)\n",
    "\n",
    "    def getFamVariants(self, fam, style = None):\n",
    "        if style is None:\n",
    "            return [item for idx, item in enumerate(self.variants) if idx in self.famvaridx[fam]]\n",
    "        elif style == \"map\":\n",
    "            names = []\n",
    "            pos = []\n",
    "            mafs = []\n",
    "            tmp_vars = self.famvaridx[fam]\n",
    "            if len(self.freq_by_fam.keys()) != 0:\n",
    "                pop_idx=self.freq.index(self.freq_by_fam[fam])\n",
    "            for idx in tmp_vars:\n",
    "                names.append(\"V{}-{}\".format(idx, self.variants[idx][1]))\n",
    "                pos.append(self.variants[idx][1])\n",
    "                tmp_mafs=self.variants[idx][-1]\n",
    "                if type(tmp_mafs) is list:\n",
    "                    mafs.append(tmp_mafs[pop_idx])\n",
    "                else:\n",
    "                    mafs.append(tmp_mafs)\n",
    "            return names, pos, mafs\n",
    "        else:\n",
    "            raise ValueError(\"Unknown style '{}'\".format(style))\n",
    "\n",
    "    def getFamSamples(self, fam):\n",
    "        nvar = len([item for idx, item in enumerate(self.variants) if idx in self.famvaridx[fam]])\n",
    "        output = [[]] * len(self.tfam.families[fam])\n",
    "        for idx, item in enumerate(self.tfam.sort_family(fam)):\n",
    "            # sample info, first 5 columns of ped\n",
    "            output[idx] = self.tfam.samples[item][:-1]\n",
    "            # sample genotypes\n",
    "            if item in self.samples:\n",
    "                output[idx].extend(self[item])\n",
    "            else:\n",
    "                output[idx].extend([\"00\"] * nvar)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.RegionExtractor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionExtractor:\n",
    "    '''Extract given genomic region from VCF\n",
    "    converting genotypes into dictionary of\n",
    "    genotype list'''\n",
    "    def __init__(self, filename, build = env.build, chr_prefix = None, allele_freq_info = None, include_vars_file=None):\n",
    "        self.vcf = cstatgen.VCFstream(filename)\n",
    "        self.chrom = self.startpos = self.endpos = self.name = None\n",
    "        self.chr_prefix = chr_prefix\n",
    "        # name of allele frequency meta info\n",
    "        self.af_info = allele_freq_info\n",
    "        self.xchecker = PseudoAutoRegion('X', build)\n",
    "        self.ychecker = PseudoAutoRegion('Y', build)\n",
    "        self.include_vars_file = include_vars_file\n",
    "\n",
    "    def apply(self, data):\n",
    "        # Clean up\n",
    "        data.reset()\n",
    "        data.chrom = self.chrom\n",
    "        self.vcf.Extract(self.chrom, self.startpos, self.endpos)\n",
    "        varIdx = 0\n",
    "        # for each variant site\n",
    "        while (self.vcf.Next()):\n",
    "            # skip tri-allelic sites\n",
    "            if not self.vcf.IsBiAllelic():\n",
    "                with env.triallelic_counter.get_lock():\n",
    "                    env.triallelic_counter.value += 1\n",
    "                continue\n",
    "            if len(data.variants) > 0:\n",
    "                if self.vcf.GetPosition()==data.variants[-1][1]:\n",
    "                    continue\n",
    "            # check if the line's sample number matches the entire VCF sample number\n",
    "            if not self.vcf.CountSampleGenotypes() == self.vcf.sampleCount:\n",
    "                raise ValueError('Genotype and sample mismatch for region {}: {:,d} vs {:,d}'.\\\n",
    "                             format(self.name, self.vcf.CountSampleGenotypes(), self.vcf.sampleCount))\n",
    "            # valid line found, get variant info\n",
    "            try:\n",
    "                if type(self.af_info) is list:\n",
    "                    maf = []\n",
    "                    large_maf = []\n",
    "                    for pop_info in self.af_info:\n",
    "                        large_maf.append(False)\n",
    "                        try:\n",
    "                            maf.append(float(self.vcf.GetInfo(pop_info)))\n",
    "                        except ValueError:\n",
    "                            maf.append(0.0)\n",
    "                    for idx in range(len(maf)):\n",
    "                        if maf[idx] > 0.5:\n",
    "                            large_maf[idx]=True\n",
    "                            maf[idx] = 1-maf[idx]\n",
    "                else:\n",
    "                    large_maf=False\n",
    "                    try:\n",
    "                        maf = float(self.vcf.GetInfo(self.af_info)) if self.af_info else None\n",
    "                    except ValueError:\n",
    "                        maf = 0.0\n",
    "                    if maf > 0.5:\n",
    "                        large_maf=True\n",
    "                        maf = 1 - maf\n",
    "            except Exception:\n",
    "                raise ValueError(\"VCF line {}:{} does not have valid allele frequency field {}!\".\\\n",
    "                                 format(self.vcf.GetChrom(), self.vcf.GetPosition(), self.af_info))\n",
    "            data.variants.append([self.vcf.GetChrom(), self.vcf.GetPosition(), self.name, maf])\n",
    "            # for each family assign member genotype if the site is non-trivial to the family\n",
    "            for k in data.families:\n",
    "                gs = self.vcf.GetGenotypes(data.famsampidx[k])\n",
    "                if len(data.freq_by_fam) > 0:\n",
    "                    popidx=self.af_info.index(data.freq_by_fam[k])\n",
    "                    if large_maf[popidx]:\n",
    "                        tmpgs=[]\n",
    "                        for tmpg in gs:\n",
    "                            if tmpg=='00':\n",
    "                                tmpgs.append(tmpg)\n",
    "                            else:\n",
    "                                tmpgs.append(''.join([str(3-int(tg)) for tg in tmpg]))\n",
    "                        gs=tuple(tmpgs)\n",
    "                else:\n",
    "                    if large_maf:\n",
    "                        tmpgs=[]\n",
    "                        for tmpg in gs:\n",
    "                            if tmpg=='00':\n",
    "                                tmpgs.append(tmpg)\n",
    "                            else:\n",
    "                                tmpgs.append(''.join([str(3-int(tg)) for tg in tmpg]))\n",
    "                        gs=tuple(tmpgs)\n",
    "                for person, g in zip(data.families[k], gs):\n",
    "                    data.genotype_all[person].append(g)\n",
    "                if len(set(''.join(gs))) <= 1:\n",
    "                    # skip monomorphic gs\n",
    "                    continue\n",
    "                else:\n",
    "                    if len(set(''.join([x for x in gs if x != \"00\"]))) <= 1:\n",
    "                        data.wtvar[k].append(varIdx)\n",
    "                    # this variant is found in the family\n",
    "                    data.famvaridx[k].append(varIdx)\n",
    "                    for person, g in zip(data.families[k], gs):\n",
    "                        data[person].append(g)\n",
    "            varIdx += 1\n",
    "        #\n",
    "        if varIdx == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            if not self.include_vars_file is None:\n",
    "                with open(self.include_vars_file) as invar_fh:\n",
    "                    for invar_line in invar_fh:\n",
    "                        chrom, pos = invar_line.split()\n",
    "                        for vidx,v in enumerate(data.variants):\n",
    "                            if v[0] == chrom and v[1] == int(pos):\n",
    "                                data.include_vars.append(\"{}\".format(pos))\n",
    "                                break\n",
    "            else:\n",
    "                data.include_vars = [\"{}\".format(item[1]) for item in data.variants]\n",
    "            with env.variants_counter.get_lock():\n",
    "                env.variants_counter.value += varIdx\n",
    "            return 0\n",
    "\n",
    "\n",
    "    def getRegion(self, region):\n",
    "        self.chrom, self.startpos, self.endpos, self.name = region[:4]\n",
    "        self.startpos = int(self.startpos)\n",
    "        self.endpos = int(self.endpos) + 1\n",
    "        if self.chrom in ['X','23']:\n",
    "            if self.xchecker.check(self.startpos) or self.xchecker.check(self.endpos):\n",
    "                self.chrom = 'XY'\n",
    "        if self.chrom in ['Y','24']:\n",
    "            if self.ychecker.check(self.startpos) or self.ychecker.check(self.endpos):\n",
    "                self.chrom = 'XY'\n",
    "        if self.chr_prefix and not self.chrom.startswith(self.chr_prefix):\n",
    "            self.chrom = self.chr_prefix + self.chrom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.MarkerMaker class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkerMaker:\n",
    "    def __init__(self, wsize, maf_cutoff = None,single_markers=False,recomb_max = 1,af_info=None,freq_by_fam=False,rsq=0.0,mle=False,rvhaplo=False,recomb_perfam=True):\n",
    "        self.missings = (\"0\", \"0\")\n",
    "        self.gtconv = {'1':0, '2':1}\n",
    "        self.recomb_max = recomb_max\n",
    "        self.haplotyper = cstatgen.HaplotypingEngine(verbose = env.debug)\n",
    "        self.af_info = af_info\n",
    "        self.freq_by_fam = freq_by_fam\n",
    "        self.rsq=rsq\n",
    "        self.mle=mle          #use MLE estimate from families for MAF\n",
    "        self.count= not mle   #count founder alleles to estimate MAF\n",
    "        self.rvhaplo=rvhaplo\n",
    "        self.recomb_perfam=recomb_perfam\n",
    "        if wsize == 0 or wsize >= 1:\n",
    "            self.r2 = None\n",
    "        else:\n",
    "            self.r2 = wsize\n",
    "        self.coder = cstatgen.HaplotypeCoder(wsize)\n",
    "        self.maf_cutoff = maf_cutoff\n",
    "        self.single_markers = single_markers\n",
    "        self.name = None\n",
    "\n",
    "    def apply(self, data):\n",
    "        # temp raw haplotype, maf and variant names data\n",
    "        haplotypes = OrderedDict()\n",
    "        mafs = {}   ##Per fam per variant\n",
    "        uniq_vars = []\n",
    "        exclude_vars = []\n",
    "        varnames = {}\n",
    "        recombPos = {}\n",
    "        #try:\n",
    "            # haplotyping plus collect found allele counts\n",
    "            # and computer founder MAFS\n",
    "        self.__Haplotype(data, haplotypes, mafs, varnames,recombPos,uniq_vars,exclude_vars)\n",
    "        print('__Haplotype',haplotypes, mafs, varnames,recombPos,uniq_vars,exclude_vars)\n",
    "        self.haplotypes, self.mafs, self.varnames = haplotypes, mafs, varnames  ###anno\n",
    "        if len(varnames):\n",
    "            if not any ([len(varnames[x]) - 1 for x in varnames]):\n",
    "                # all families have only one variant\n",
    "                self.__AssignSNVHaplotypes(data, haplotypes, mafs, varnames)\n",
    "            else:\n",
    "                # calculate LD clusters using founder haplotypes\n",
    "                clusters = self.__ClusterByLD(data, haplotypes, varnames)\n",
    "                # recoding the genotype of the region\n",
    "                self.__CodeHaplotypes(data, haplotypes, mafs, varnames, clusters)\n",
    "    #except Exception as e:\n",
    "        #    if env.debug:\n",
    "        #        raise\n",
    "        #    return -1\n",
    "        self.__FormatHaplotypes(data,recombPos,varnames,uniq_vars)\n",
    "        return 0\n",
    "\n",
    "    def __getMLEfreq(self,data, markers_to_analyze, pos_all, families, rsq, output_log):\n",
    "        output_sample=[]\n",
    "        mle_mafs={}\n",
    "        if len(markers_to_analyze)==0:\n",
    "            return mle_mafs\n",
    "        for fam in families:\n",
    "            for person in data.tfam.sort_family(fam):\n",
    "                output_sample.append([])\n",
    "                last_ele=len(output_sample)-1\n",
    "                output_sample[last_ele] = data.tfam.samples[person][:-1]\n",
    "                if person in data.samples:\n",
    "                    for marker in markers_to_analyze:\n",
    "                        idx=int(marker.split('-')[0][1:])\n",
    "                        output_sample[last_ele].append(data.genotype_all[person][idx])\n",
    "                else:\n",
    "                    output_sample[last_ele].extend([\"00\"] * len(markers_to_analyze))\n",
    "        with stdoutRedirect(to = output_log):\n",
    "            af = self.haplotyper.Execute(data.chrom, markers_to_analyze, pos_all, output_sample, rsq, output_log,False)\n",
    "        with open(output_log) as mle_fh:\n",
    "            for line in mle_fh:\n",
    "                if line.startswith('V'):\n",
    "                    tmp_eles = line.split(':')\n",
    "                    if tmp_eles[0] not in mle_mafs:\n",
    "                        freqs=tmp_eles[1].split()\n",
    "                        mle_maf = float(freqs[1])\n",
    "                        if mle_maf>0.5:\n",
    "                            mle_mafs[tmp_eles[0]]=float(\"%.9f\"%(1-mle_maf))\n",
    "                        else:\n",
    "                            #alt allele is more frequent\n",
    "                            mle_mafs[tmp_eles[0]]=float(\"%.9f\"%mle_maf)\n",
    "                            marker_idx=int(tmp_eles[0].split('-')[0][1:])\n",
    "                            for fam in families:\n",
    "                                if marker_idx not in data.famvaridx[fam]:\n",
    "                                    continue\n",
    "                                tmp_famvaridx=data.famvaridx[fam].index(marker_idx)\n",
    "                                for person in data.families[fam]:\n",
    "                                    tmpg=data.genotype_all[person][marker_idx]\n",
    "                                    tmpg_switch=''.join([str(3-int(tg)) for tg in tmpg]) if tmpg!='00' else tmpg\n",
    "                                    data.genotype_all[person][marker_idx]=tmpg_switch\n",
    "                                    tmpg2=data[person][tmp_famvaridx]\n",
    "                                    tmpg_switch2=''.join([str(3-int(tg)) for tg in tmpg2]) if tmpg2!='00' else tmpg2\n",
    "                                    data[person][tmp_famvaridx]=tmpg_switch2\n",
    "        return mle_mafs\n",
    "\n",
    "    def __computefounderfreq(self,data, families):\n",
    "        #count founder alleles to estimate MAF\n",
    "        total_founder_alleles=0\n",
    "        tmp_haplotypes=OrderedDict()\n",
    "        tmp_mafs={}\n",
    "        for item in families:\n",
    "            tmp_haplotypes[item] = self.__PedToHaplotype(data.getFamSamples(item))\n",
    "            # count founder alleles\n",
    "            for hap in tmp_haplotypes[item]:\n",
    "                if not data.tfam.is_founder(hap[1]):\n",
    "                    continue\n",
    "                total_founder_alleles+=1.0\n",
    "                for idxv, v in enumerate(data.getFamVariants(item,style=\"map\")[0]):\n",
    "                    if v not in tmp_mafs:\n",
    "                        # [#alt, #haplotypes]\n",
    "                        tmp_mafs[v] = [0, 0]\n",
    "                    gt = hap[2 + idxv][1] if hap[2 + idxv][0].isupper() else hap[2 + idxv][0]\n",
    "                    if not gt == \"?\":\n",
    "                    #genotyped\n",
    "                        tmp_mafs[v][0] += self.gtconv[gt]\n",
    "                    else:\n",
    "                    #genotype is missing\n",
    "                        tmp_mafs[v][1] -= 1.0\n",
    "        #compute MAFs based on counts\n",
    "        for v in tmp_mafs:\n",
    "            if type(tmp_mafs[v]) is not list:\n",
    "                continue\n",
    "            tmp_mafs[v] = tmp_mafs[v][0] / (tmp_mafs[v][1]+total_founder_alleles) if tmp_mafs[v][1]+total_founder_alleles > 0 else 0.0\n",
    "        return tmp_mafs\n",
    "\n",
    "    def __Haplotype(self, data, haplotypes, mafs, varnames,recombPos,uniq_vars,exclude_vars):\n",
    "        '''genetic haplotyping. haplotypes stores per family data'''\n",
    "        # FIXME: it is SWIG's (2.0.12) fault not to properly destroy the object \"Pedigree\" in \"Execute()\"\n",
    "        # So there is a memory leak here which I tried to partially handle on C++\n",
    "        #\n",
    "        # Per family haplotyping\n",
    "        #\n",
    "        self.markers = [\"V{}-{}\".format(idx, item[1]) for idx, item in enumerate(data.variants)]\n",
    "        for item in data.families:\n",
    "            varnames[item], positions, vcf_mafs = data.getFamVariants(item, style = \"map\")\n",
    "            if len(varnames[item]) == 0:\n",
    "                for person in data.families[item]:\n",
    "                    data[person] = self.missings\n",
    "                continue\n",
    "            if env.debug:\n",
    "                with env.lock:\n",
    "                    sys.stderr.write('\\n'.join(['\\t'.join(x) for x in data.getFamSamples(item)]) + '\\n\\n')\n",
    "            # haplotyping\n",
    "            self.hap = {}\n",
    "            with env.lock:\n",
    "                if not env.prephased:\n",
    "                    tmp_log_output=env.tmp_log + str(os.getpid()) \n",
    "                    #with stdoutRedirect(to = tmp_log_output + '.log'):\n",
    "                    haplotypes[item] = self.haplotyper.Execute(data.chrom, varnames[item], sorted(positions), \n",
    "                                                                   data.getFamSamples(item), self.rsq, tmp_log_output)[0]\n",
    "                    print('haplotyper execute',item,haplotypes[item])\n",
    "                    self.hap[item] = haplotypes[item]\n",
    "                else:\n",
    "                    haplotypes[item] = self.__PedToHaplotype(data.getFamSamples(item))\n",
    "           \n",
    "            if len(haplotypes[item]) == 0:\n",
    "                # C++ haplotyping implementation failed\n",
    "                with env.chperror_counter.get_lock():\n",
    "                    env.chperror_counter.value += 1\n",
    "            # either use privided MAF or computer MAF\n",
    "            if all(vcf_mafs):\n",
    "                for idx, v in enumerate(varnames[item]):\n",
    "                    if v not in mafs:\n",
    "                        mafs[v] = vcf_mafs[idx]\n",
    "            else:\n",
    "                # count founder alleles\n",
    "                for hap in haplotypes[item]:\n",
    "                    if not data.tfam.is_founder(hap[1]):\n",
    "                        continue\n",
    "                    for idxv, v in enumerate(varnames[item]):\n",
    "                        if v not in mafs:\n",
    "                            # [#alt, #haplotypes]\n",
    "                            mafs[v] = [0, 0]\n",
    "                        gt = hap[2 + idxv][1] if hap[2 + idxv][0].isupper() else hap[2 + idxv][0]\n",
    "                        if not gt == \"?\":\n",
    "                            mafs[v][0] += self.gtconv[gt]\n",
    "                            mafs[v][1] += 1.0\n",
    "        #\n",
    "        # Compute founder MAFs\n",
    "        #\n",
    "        for v in mafs:\n",
    "            if type(mafs[v]) is not list:\n",
    "                continue\n",
    "            mafs[v] = mafs[v][0] / mafs[v][1] if mafs[v][1] > 0 else 0.0\n",
    "        if env.debug:\n",
    "            with env.lock:\n",
    "                print(\"variant mafs = \", mafs, \"\\n\", file = sys.stderr)\n",
    "        #\n",
    "        # Drop some variants if maf is greater than given threshold\n",
    "        #\n",
    "        if self.maf_cutoff is not None:\n",
    "            exclude_vars = []\n",
    "            for v in mafs.keys():\n",
    "                if mafs[v] > self.maf_cutoff:\n",
    "                    exclude_vars.append(v)\n",
    "            for i in haplotypes.keys():\n",
    "                haplotypes[i] = listit(haplotypes[i])\n",
    "                for j in range(len(haplotypes[i])):\n",
    "                    haplotypes[i][j] = haplotypes[i][j][:2] + \\\n",
    "                      [x for idx, x in enumerate(haplotypes[i][j][2:]) if varnames[i][idx] not in exclude_vars]\n",
    "                varnames[i] = [x for x in varnames[i] if x not in exclude_vars]\n",
    "                # handle trivial data\n",
    "                if len(varnames[i]) == 0:\n",
    "                    for person in data.families[i]:\n",
    "                        data[person] = self.missings\n",
    "                    del varnames[i]\n",
    "                    del haplotypes[i]\n",
    "            # count how many variants are removed\n",
    "            with env.commonvar_counter.get_lock():\n",
    "                env.commonvar_counter.value += len(exclude_vars)\n",
    "\n",
    "    def __ClusterByLD(self, data, haplotypes, varnames):\n",
    "        if self.r2 is None:\n",
    "            return None\n",
    "        # get founder haplotypes\n",
    "        founder_haplotypes = []\n",
    "        markers = sorted(set(itertools.chain(*varnames.values())), key = lambda x: int(x.split(\"-\")[0][1:]))\n",
    "        for item in haplotypes:\n",
    "            for ihap, hap in enumerate(haplotypes[item]):\n",
    "                if not data.tfam.is_founder(hap[1]):\n",
    "                    continue\n",
    "                gt = [hap[2 + varnames[item].index(v)] if v in varnames[item] else '?' for v in markers]\n",
    "                founder_haplotypes.append((\"{}-{}\".format(hap[1], ihap % 2), \"\".join([x[1] if x[0].isupper() else x[0] for x in gt])))\n",
    "        # calculate LD blocks, use r2 measure\n",
    "        ld = Align.create(founder_haplotypes).matrixLD(validCharacters=\"12\")[\"r2\"]\n",
    "        blocks = []\n",
    "        for j in ld:\n",
    "            block = [j]\n",
    "            for k in ld[j]:\n",
    "                if ld[j][k] > self.r2:\n",
    "                    block.append(k)\n",
    "            if len(block) > 1:\n",
    "                blocks.append(block)\n",
    "        self.ld, self.blocks = ld, blocks\n",
    "        # get LD clusters\n",
    "        clusters = [[markers[idx] for idx in item] for item in list(connected_components(blocks))]\n",
    "        if env.debug:\n",
    "            with env.lock:\n",
    "                print(\"LD blocks: \", blocks, file = sys.stderr)\n",
    "                print(\"LD clusters: \", clusters, file = sys.stderr)\n",
    "        return clusters\n",
    "\n",
    "\n",
    "    def __CodeHaplotypes(self, data, haplotypes, mafs, varnames, clusters):\n",
    "        # apply CHP coding\n",
    "        for item in data.famvaridx:\n",
    "            if item not in haplotypes and data[data.families[item][0]] != ('0','0'):\n",
    "                # when only wild-type haplotypes are present in a family, still code them instead of ignoring the family\n",
    "                if self.freq_by_fam:\n",
    "                    pop=data.freq_by_fam[item]\n",
    "                    try:\n",
    "                        varnames[item]=data.total_varnames[pop]\n",
    "                        mafs[item]=data.total_mafs[pop]\n",
    "                    except:\n",
    "                        continue\n",
    "                else:\n",
    "                    varnames[item]=data.total_varnames['pop']\n",
    "                    mafs[item]=data.total_mafs\n",
    "                haplotypes[item]=[]\n",
    "                for person in data.families[item]:\n",
    "                    tmp_person=[item, person]\n",
    "                    if '00' in data[person]:\n",
    "                        tmp_person+=['?:']*len(varnames[item])\n",
    "                    else:\n",
    "                        tmp_person+=['1:']*len(varnames[item])\n",
    "                    haplotypes[item].append(tmp_person)\n",
    "                    haplotypes[item].append(tmp_person)\n",
    "            elif item in haplotypes:\n",
    "                nonvar_hap_flag=False\n",
    "                #determine if wild-type haplotype is present in a family\n",
    "                for hap in haplotypes[item]:\n",
    "                    tmp_genes=[]\n",
    "                    for tmpa in hap[2:]:\n",
    "                        if 'A' in tmpa or 'B' in tmpa:\n",
    "                            tmp_genes.append(tmpa[1])\n",
    "                        else:\n",
    "                            tmp_genes.append(tmpa[0])\n",
    "                    if set(tmp_genes)==set(['1']):\n",
    "                        #non variant haplotype\n",
    "                        nonvar_hap_flag=True\n",
    "                        break\n",
    "                if not nonvar_hap_flag:\n",
    "                    #if family don't have wild-type haplotype, add a fake one to ensure correct coding\n",
    "                    var_num=len(varnames[item])\n",
    "                    fake_person=[item, 'FAKEPERSON']+['1:']*var_num\n",
    "                    haplotypes[item].append(fake_person)\n",
    "                for hidx,hap in enumerate(haplotypes[item]):\n",
    "                    if hap[1] in data.missing_persons:\n",
    "                        missing_person=[item,hap[1]]+['?:']*len(varnames[item])\n",
    "                        haplotypes[item][hidx]=missing_person\n",
    "\n",
    "        if not clusters is None:\n",
    "            clusters_idx = [[[varnames[item].index(x) for x in y] for y in clusters] for item in haplotypes]\n",
    "        else:\n",
    "            clusters_idx = [[[]] for item in haplotypes]\n",
    "        if env.debug:\n",
    "            for item in haplotypes:\n",
    "                with env.lock:\n",
    "                    print(varnames[item],file=sys.stderr)\n",
    "                    print(\"hap{0}\\t{1}\\n\".format(item,haplotypes[item]),file=sys.stderr)\n",
    "        self.coder.Execute(haplotypes.values(), [[mafs[item][v] for v in varnames[item]] for item in haplotypes], clusters_idx)\n",
    "        if env.debug:\n",
    "            with env.lock:\n",
    "                if clusters:\n",
    "                    print(\"Family LD clusters: \", clusters_idx, \"\\n\", file = sys.stderr)\n",
    "                self.coder.Print()\n",
    "        # line: [fid, sid, hap1, hap2]\n",
    "        for line in self.coder.GetHaplotypes():\n",
    "            if not line[1] in data:\n",
    "                # this sample is not in VCF file. Every variant site should be missing\n",
    "                # they have to be skipped for now\n",
    "                continue\n",
    "            data[line[1]] = (line[2].split(','), line[4].split(','))\n",
    "            #sub-region count for each sample individual\n",
    "            superMarkerCount=len(data[line[1]][0])\n",
    "            if line[0] not in data.patterns:\n",
    "                data.patterns[line[0]]=[[] for x in range(superMarkerCount)]\n",
    "            for t_Marker in range(superMarkerCount):\n",
    "                t_pat1=line[3].split(',')[t_Marker]\n",
    "                t_pat2=line[5].split(',')[t_Marker]\n",
    "                if t_pat1 not in data.patterns[line[0]][t_Marker]:\n",
    "                    data.patterns[line[0]][t_Marker].append(t_pat1)\n",
    "                if t_pat2 not in data.patterns[line[0]][t_Marker]:\n",
    "                    data.patterns[line[0]][t_Marker].append(t_pat2)\n",
    "            if len(data[line[1]][0]) > data.superMarkerCount:\n",
    "                data.superMarkerCount = len(data[line[1]][0])\n",
    "        # get MAF\n",
    "        for item in data.famvaridx:\n",
    "            if item not in haplotypes:\n",
    "                for person in data.families[item]:\n",
    "                    data[person]=(['0']*data.superMarkerCount,['0']*data.superMarkerCount)\n",
    "        for item in haplotypes:\n",
    "            data.maf[item] = self.coder.GetAlleleFrequencies(item)\n",
    "            if not len(data.maf[item][0]):\n",
    "                continue\n",
    "            data.varnames_by_fam[item]=varnames[item]\n",
    "            wt_maf=0\n",
    "            if self.freq_by_fam:\n",
    "                try:\n",
    "                    wt_maf=data.wt_maf[data.freq_by_fam[item]]\n",
    "                except:\n",
    "                    pass\n",
    "            else:\n",
    "                wt_maf=data.wt_maf['pop']\n",
    "            tmp_data_maf=[]\n",
    "            for v in data.maf[item]:\n",
    "                if len(v)==1:\n",
    "                    tmp_data_maf.append((v[0],1-v[0]))\n",
    "                else:\n",
    "                    if np.sum(v)<1:\n",
    "                        tmp_ratio=sum(v[1:])/(1-wt_maf)\n",
    "                        tmp_list=[wt_maf]\n",
    "                        if tmp_ratio==0:\n",
    "                            tmp_list.append(1-wt_maf)\n",
    "                        else:\n",
    "                            for tmpv in v[1:]:\n",
    "                                tmp_list.append(tmpv/tmp_ratio)\n",
    "                        tmp_data_maf.append(tuple(tmp_list))\n",
    "                    else:\n",
    "                        tmp_data_maf.append(v)\n",
    "            data.maf[item]=tuple(tmp_data_maf)\n",
    "        if env.debug:\n",
    "            with env.lock:\n",
    "                print(\"marker freqs = \", data.maf, \"\\n\", file = sys.stderr)\n",
    "\n",
    "\n",
    "    def __AssignSNVHaplotypes(self, data, haplotypes, mafs, varnames):\n",
    "        for item in haplotypes:\n",
    "            # each person's haplotype\n",
    "            data.varnames_by_fam[item]=varnames[item]\n",
    "            token = ''\n",
    "            for idx,line in enumerate(haplotypes[item]):\n",
    "                if line[1] in data.missing_persons:\n",
    "                    data[line[1]]=('0','0')\n",
    "                else:\n",
    "                    if not idx % 2:\n",
    "                        token = line[2][1] if line[2][0].isupper() else line[2][0]\n",
    "                        if token=='?':\n",
    "                            token='0'\n",
    "                    else:\n",
    "                        tmp_token = line[2][1] if line[2][0].isupper() else line[2][0]\n",
    "                        if tmp_token=='?':\n",
    "                            tmp_token='0'\n",
    "                        data[line[1]] = (token, tmp_token)\n",
    "\n",
    "            # get MAF\n",
    "            data.maf[item] = [(1 - mafs[item][varnames[item][0]], mafs[item][varnames[item][0]])]\n",
    "            data.maf[item] = tuple(tuple(np.array(v) / np.sum(v)) if np.sum(v) else v\n",
    "                              for v in data.maf[item])\n",
    "        for item in data.famvaridx:\n",
    "            if item not in haplotypes and data[data.families[item][0]] != ('0','0'):\n",
    "                for person in data.families[item]:\n",
    "                    if '00' in data[person]:\n",
    "                        data[person]=('0','0')\n",
    "                    else:\n",
    "                        data[person]=('1','1')\n",
    "                t_maf=0\n",
    "                if self.freq_by_fam:\n",
    "                    try:\n",
    "                        t_maf=data.wt_maf[data.freq_by_fam[item]]\n",
    "                    except:\n",
    "                        for person in data.families[item]:\n",
    "                            data[person]=('0','0')\n",
    "                else:\n",
    "                    t_maf=data.wt_maf['pop']\n",
    "                data.maf[item]=((t_maf,1-t_maf),)\n",
    "        if env.debug:\n",
    "            with env.lock:\n",
    "                print(\"marker freqs = \", data.maf, \"\\n\", file = sys.stderr)\n",
    "\n",
    "\n",
    "    def __FormatHaplotypes(self, data,recombPos,varnames,uniq_vars):\n",
    "        # Reformat sample genotypes\n",
    "        ## Linhai Edit: Reformat to deal with recombination events in families\n",
    "        if self.recomb_perfam:\n",
    "            #code recombination per family basis, no need to consider overlap across families\n",
    "            for person in data:\n",
    "                if type(data[person]) is not tuple:\n",
    "                    data[person] = self.missings\n",
    "                    continue\n",
    "                diff = data.superMarkerCount - len(data[person][0])\n",
    "                data[person] = zip(*data[person])\n",
    "                if diff > 0:\n",
    "                    data[person].extend([self.missings] * diff)\n",
    "        else:\n",
    "            #code recombination across families to generate sub-regions that extend across families\n",
    "            tmp_combined_recombPos={}\n",
    "            sorted_var = sorted(uniq_vars, key=lambda x: int(x.split('-')[0][1:]))\n",
    "            for fam in data.maf.keys():\n",
    "                if len(data.maf[fam])>1:\n",
    "                    for pair in sorted(recombPos[fam].keys(), key=lambda x:(sorted_var.index(x[0]),sorted_var.index(x[1]))):\n",
    "                        if pair[1] == varnames[fam][0]:\n",
    "                            ##remove recombination event if occurred at 1st RV\n",
    "                            del recombPos[fam][pair]\n",
    "                            continue\n",
    "                        if fam not in tmp_combined_recombPos:\n",
    "                            tmp_combined_recombPos[fam]=[pair]\n",
    "                        else:\n",
    "                            tmp_combined_recombPos[fam].append(pair)\n",
    "            tmp_all_recombs=[pair for pairs in tmp_combined_recombPos.values() for pair in pairs]\n",
    "            sorted_combined_recombPos=sorted(list(set(tmp_all_recombs)),key=lambda x:(sorted_var.index(x[0]),sorted_var.index(x[1])))\n",
    "            recomb_fams=tmp_combined_recombPos.keys()\n",
    "            ##get sub-regions that applies to all families\n",
    "            for varidx,variant in enumerate(sorted_var):\n",
    "                included_fams=len(recomb_fams)\n",
    "                for recomb_region in sorted_combined_recombPos:\n",
    "                    if varidx > sorted_var.index(recomb_region[0]) and varidx < sorted_var.index(recomb_region[1]):\n",
    "                        ##if the variant is in a recombination region\n",
    "                        included_fams-=1\n",
    "                if included_fams==len(recomb_fams):\n",
    "                    if data.combined_regions==[]:\n",
    "                        data.combined_regions.append([variant])\n",
    "                    else:\n",
    "                        if sorted_var.index(data.combined_regions[-1][-1])==varidx-1:\n",
    "                            neighbour_recomb_flag=False\n",
    "                            for recomb_region in sorted_combined_recombPos:\n",
    "                                recomb_idx=sorted_var.index(recomb_region[1])\n",
    "                                if recomb_idx==varidx:\n",
    "                                    neighbour_recomb_flag=True\n",
    "                                    break\n",
    "                                elif recomb_idx>varidx:\n",
    "                                    break\n",
    "                            if neighbour_recomb_flag:\n",
    "                                data.combined_regions.append([variant])\n",
    "                            else:\n",
    "                                data.combined_regions[-1].append(variant)\n",
    "                        else:\n",
    "                            data.combined_regions.append([variant])\n",
    "            ##Get the markers in families compliant with the sub_regions\n",
    "            for sub_region in data.combined_regions:\n",
    "                markers={}\n",
    "                for fam in recomb_fams:\n",
    "                    pidx=0\n",
    "                    for pair in sorted(recombPos[fam].keys(), key=lambda x:(sorted_var.index(x[0]),sorted_var.index(x[1]))):\n",
    "                        sub_region_start=sorted_var.index(sub_region[0])\n",
    "                        sub_region_end=sorted_var.index(sub_region[-1])\n",
    "                        recomb_start=sorted_var.index(pair[0])\n",
    "                        recomb_end=sorted_var.index(pair[1])\n",
    "                        if sub_region_end <= recomb_start:\n",
    "                            markers[fam]=pidx\n",
    "                            break\n",
    "                        elif sub_region_end > recomb_start and sub_region_start>recomb_start and sub_region_end<recomb_end:\n",
    "                            ##within the recombination region\n",
    "                            markers[fam]=None\n",
    "                            break\n",
    "                        pidx+=1\n",
    "                    if fam not in markers:\n",
    "                        markers[fam]=pidx\n",
    "                data.complied_markers.append(markers)\n",
    "            data.superMarkerCount=len(data.combined_regions)\n",
    "            #coordinates for sub_regions\n",
    "            data.coordinates_by_region=[(int(sub_region[0].split('-')[1])+int(sub_region[-1].split('-')[1]))/2 for sub_region in data.combined_regions]\n",
    "            for person in data:\n",
    "                if type(data[person]) is not tuple:\n",
    "                    data[person] = self.missings\n",
    "                    continue\n",
    "                diff = data.superMarkerCount - len(data[person][0])\n",
    "                data[person] = zip(*data[person])\n",
    "                if diff > 0:\n",
    "                    if len(data[person]) == 1:\n",
    "                        ##only one whole region with no recombination\n",
    "                        data[person].extend(data[person] * diff)\n",
    "                    else:\n",
    "                        famid=''\n",
    "                        for fam in data.complied_markers[0].keys():\n",
    "                            if person in data.families[fam]:\n",
    "                                famid=fam\n",
    "                        complied_data=[]\n",
    "                        for marker in data.complied_markers:\n",
    "                            complied_data.append(data[person][marker[famid]])\n",
    "                        data[person]=complied_data\n",
    "\n",
    "    def __PedToHaplotype(self, ped):\n",
    "        '''convert prephased ped format to haplotype format.\n",
    "        Input: e.g. [['13346', '5888', '0', '0', '1', '11', '11', '11'], ['13346', '5856', '0', '0', '2', '12', '12', '12'], ['13346', '5920', '5888', '5856', '1', '12', '12', '12'], ['13346', '6589', '5888', '5856', '1', '11', '11', '11']]\n",
    "        Output: e.g. (('13346', '5856', '1:', '1:', '1:'), ('13346', '5856', '2:', '2:', '2:'), ('13346', '5888', '1:', '1:', '1:'), ('13346', '5888', '1:', '1:', '1:'), ('13346', '6589', '1:', '1|', '1|'), ('13346', '6589', '1:', '1|', '1|'), ('13346', '5920', '2:', '2|', '2|'), ('13346', '5920', '1:', '1|', '1|'))\n",
    "        '''\n",
    "        haps = []\n",
    "        for item in ped:\n",
    "            entry = [item[0], item[1]] + [x[0] + ':' if x[0] != '0' else '?:' for x in item[5:]]\n",
    "            haps.append(tuple(entry))\n",
    "            entry = [item[0], item[1]] + [x[1] + ':' if x[1] != '0' else '?:' for x in item[5:]]\n",
    "            haps.append(tuple(entry))\n",
    "        return tuple(haps)\n",
    "\n",
    "    def getRegion(self, region):\n",
    "        self.name = region[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.LinkageWriter class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkageWriter:\n",
    "    def __init__(self, num_missing_append = 0):\n",
    "        self.chrom = self.prev_chrom = self.name = self.distance = self.distance_avg = self.distance_m = self.distance_f = None\n",
    "        self.distance_by_region=[]\n",
    "        self.mid_position=None\n",
    "        self.reset()\n",
    "        self.missings = [\"0\", \"0\"]\n",
    "        self.num_missing = num_missing_append\n",
    "\n",
    "    def apply(self, data):\n",
    "        if self.chrom != self.prev_chrom:\n",
    "            if self.prev_chrom is None:\n",
    "                self.prev_chrom = self.chrom\n",
    "            else:\n",
    "                # new chrom entered,\n",
    "                # commit whatever is in buffer before accepting new data\n",
    "                self.commit()\n",
    "        # write tped output\n",
    "        position = str(data.getMidPosition())\n",
    "        if data.superMarkerCount <= 1:\n",
    "            # genotypes\n",
    "            gs = [data[s][0] for s in data.samples]\n",
    "            if len(set(gs)) == 1:\n",
    "                # everyone's genotype is the same (most likely missing or monomorphic)\n",
    "                return 2\n",
    "            self.tped += env.delimiter.join([self.chrom, self.name, self.distance, position] + \\\n",
    "                list(itertools.chain(*gs)) + self.missings*self.num_missing) + \"\\n\"\n",
    "            # freqs\n",
    "            for k in data.maf:\n",
    "                self.freq += env.delimiter.join([k, self.name] + map(str, data.maf[k][0])) + \"\\n\"\n",
    "        else:\n",
    "            # have to expand each region into mutiple chunks to account for different recomb points\n",
    "            gs = zip(*[data[s] for s in data.samples])\n",
    "            # sub-chunk id\n",
    "            cid = 0\n",
    "            skipped_chunk = []\n",
    "            self.distance_by_region=[self.distance_converter(x,int(position)) for x in data.coordinates_by_region]\n",
    "            for idx, g in enumerate(gs):\n",
    "                if len(set(g)) == 1:\n",
    "                    skipped_chunk.append(idx)\n",
    "                    continue\n",
    "                cid += 1\n",
    "                self.tped += \\\n",
    "                  env.delimiter.join([self.chrom, '{}[{}]'.format(self.name, cid), self.distance_by_region[cid-1], position] + \\\n",
    "                  list(itertools.chain(*g)) + self.missings*self.num_missing) + \"\\n\"\n",
    "            if cid == 0:\n",
    "                # everyone's genotype is the same (most likely missing or monomorphic)\n",
    "                return 2\n",
    "            # freqs\n",
    "            for k in data.maf:\n",
    "                cid = 0\n",
    "                for idx in range(data.superMarkerCount):\n",
    "                    if idx in skipped_chunk:\n",
    "                        continue\n",
    "                    if not data.complied_markers:\n",
    "                        #if recombination coded per family instead of across families\n",
    "                        if idx >= len(data.maf[k]):\n",
    "                            break\n",
    "                        cid += 1\n",
    "                        self.freq += env.delimiter.join([k, '{}[{}]'.format(self.name, cid)] + \\\n",
    "                                                    map(str, data.maf[k][idx])) + \"\\n\"\n",
    "                    else:\n",
    "                        if len(data.maf[k])>1:\n",
    "                            matched_idx=data.complied_markers[idx][k]\n",
    "                            cid += 1\n",
    "                            self.freq += env.delimiter.join([k, '{}[{}]'.format(self.name, cid)] + \\\n",
    "                                                map(str, data.maf[k][matched_idx])) + \"\\n\"\n",
    "                        elif len(data.maf[k])==1:\n",
    "                            cid += 1\n",
    "                            self.freq += env.delimiter.join([k, '{}[{}]'.format(self.name, cid)] + \\\n",
    "                                                map(str, data.maf[k][0])) + \"\\n\"\n",
    "        if data.combined_regions:\n",
    "            self.chp += \"CHP Super Marker positions: \"+repr(data.combined_regions)+\"\\n\"\n",
    "        for item in data.varnames_by_fam:\n",
    "            try:\n",
    "                pattern_txt=[tuple(sorted(data.patterns[item][tmarker],key=lambda x:x.count('2') )) for tmarker in range(len(data.patterns[item]))]\n",
    "            except:\n",
    "                pattern_txt=''\n",
    "            self.varfam += \"{}\\t{}\\t{}\\n\".format(item,data.varnames_by_fam[item],pattern_txt)\n",
    "        if self.counter < env.batch:\n",
    "            self.counter += data.superMarkerCount\n",
    "        else:\n",
    "            self.commit()\n",
    "        return 0\n",
    "\n",
    "    def commit(self):\n",
    "        if self.tped:\n",
    "            with env.lock:\n",
    "                with open(os.path.join(env.tmp_cache, '{}.chr{}.tped'.format(env.output, self.prev_chrom)),\n",
    "                          'a') as f:\n",
    "                    f.write(self.tped)\n",
    "        if self.freq:\n",
    "            with env.lock:\n",
    "                with open(os.path.join(env.tmp_cache, '{}.chr{}.freq'.format(env.output, self.prev_chrom)),\n",
    "                          'a') as f:\n",
    "                    f.write(self.freq)\n",
    "        if self.chp:\n",
    "            with env.lock:\n",
    "                with open(os.path.join(env.tmp_cache, '{}.chr{}.chp'.format(env.output, self.prev_chrom)),\n",
    "                          'a') as f:\n",
    "                    f.write(self.chp)\n",
    "        if self.varfam:\n",
    "            with env.lock:\n",
    "                with open(os.path.join(env.tmp_cache, '{}.chr{}.var'.format(env.output, self.prev_chrom)),\n",
    "                          'a') as f:\n",
    "                    f.write(self.varfam)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.tped = ''\n",
    "        self.freq = ''\n",
    "        self.chp = ''\n",
    "        self.varfam = ''\n",
    "        self.counter = 0\n",
    "        self.prev_chrom = self.chrom\n",
    "\n",
    "    def distance_converter(self, x, mid_position):\n",
    "        delta=(x-mid_position)/1000000.0\n",
    "        distance='%.5f'%(float(self.distance_avg)+delta)\n",
    "        distance_m='%.5f'%(float(self.distance_m)+delta)\n",
    "        distance_f='%.5f'%(float(self.distance_f)+delta)\n",
    "        return \";\".join([distance,distance_m,distance_f])\n",
    "\n",
    "    def getRegion(self, region):\n",
    "        self.chrom = region[0]\n",
    "        self.name, self.distance_avg, self.distance_m, self.distance_f = region[3:]\n",
    "        self.distance = \";\".join([self.distance_avg, self.distance_m, self.distance_f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.EncoderWorker class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderWorker(Process):\n",
    "    def __init__(self, queue, length, data, extractor, coder, writer):\n",
    "        Process.__init__(self)\n",
    "        self.queue = queue\n",
    "        self.numGrps = float(length)\n",
    "        self.data = data\n",
    "        self.extractor = extractor\n",
    "        self.maker = coder\n",
    "        self.writer = writer\n",
    "\n",
    "    def report(self):\n",
    "        env.log('{:,d} units processed {{{:.2%}}} ...'.\\\n",
    "                format(env.success_counter.value, env.total_counter.value / self.numGrps), flush = True)\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            try:\n",
    "                region = self.queue.pop(0) if isinstance(self.queue, list) else self.queue.get()\n",
    "                if region is None:\n",
    "                    self.writer.commit()\n",
    "                    self.report()\n",
    "                    # total mendelian errors found\n",
    "                    with env.mendelerror_counter.get_lock():\n",
    "                        env.mendelerror_counter.value += self.maker.haplotyper.CountMendelianErrors()\n",
    "                    # total recombination events found\n",
    "                    with env.recomb_counter.get_lock():\n",
    "                        env.recomb_counter.value += self.maker.coder.CountRecombinations()\n",
    "                    break\n",
    "                else:\n",
    "                    with env.total_counter.get_lock():\n",
    "                        env.total_counter.value += 1\n",
    "                    self.extractor.getRegion(region)\n",
    "                    self.writer.getRegion(region)\n",
    "                    self.maker.getRegion(region)\n",
    "                    isSuccess = True\n",
    "                    for m in [self.extractor, self.maker, self.writer]:\n",
    "                        status = m.apply(self.data)\n",
    "                        if status == -1:\n",
    "                            with env.chperror_counter.get_lock():\n",
    "                                # previous module failed\n",
    "                                env.chperror_counter.value += 1\n",
    "                        if status == 1:\n",
    "                            with env.null_counter.get_lock():\n",
    "                                env.null_counter.value += 1\n",
    "                        if status == 2:\n",
    "                            with env.trivial_counter.get_lock():\n",
    "                                env.trivial_counter.value += 1\n",
    "                        if status != 0:\n",
    "                            isSuccess = False\n",
    "                            break\n",
    "                    if isSuccess:\n",
    "                        with env.success_counter.get_lock():\n",
    "                            env.success_counter.value += 1\n",
    "                    if env.total_counter.value % (env.batch * env.jobs) == 0:\n",
    "                        self.report()\n",
    "            except KeyboardInterrupt:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RegionExtractor:\n",
    "    '''Extract given genomic region from VCF\n",
    "    converting genotypes into dictionary of\n",
    "    genotype list'''\n",
    "    def __init__(self, filename, build = env.build, chr_prefix = None, allele_freq_info = None):\n",
    "        self.vcf = cstatgen.VCFstream(filename)\n",
    "        self.chrom = self.startpos = self.endpos = self.name = None\n",
    "        self.chr_prefix = chr_prefix\n",
    "        # name of allele frequency meta info\n",
    "        self.af_info = allele_freq_info\n",
    "        self.xchecker = PseudoAutoRegion('X', build)\n",
    "        self.ychecker = PseudoAutoRegion('Y', build)\n",
    "\n",
    "    def apply(self, data):\n",
    "        # Clean up\n",
    "        data.reset()\n",
    "        data.chrom = self.chrom\n",
    "        self.vcf.Extract(self.chrom, self.startpos, self.endpos)\n",
    "        varIdx = 0\n",
    "        # for each variant site\n",
    "        while (self.vcf.Next()):\n",
    "            # skip tri-allelic sites\n",
    "            if not self.vcf.IsBiAllelic():\n",
    "                with env.triallelic_counter.get_lock():\n",
    "                    env.triallelic_counter.value += 1\n",
    "                continue\n",
    "            # check if the line's sample number matches the entire VCF sample number\n",
    "            if not self.vcf.CountSampleGenotypes() == self.vcf.sampleCount:\n",
    "                raise ValueError('Genotype and sample mismatch for region {}: {:,d} vs {:,d}'.\\\n",
    "                             format(self.name, self.vcf.CountSampleGenotypes(), self.vcf.sampleCount))\n",
    "            # valid line found, get variant info\n",
    "            try:\n",
    "                maf = float(self.vcf.GetInfo(self.af_info)) if self.af_info else None\n",
    "                if maf > 0.5:\n",
    "                    maf = 1 - maf\n",
    "                elif maf<=0.0:\n",
    "                    maf = 0.0001    #fixme\n",
    "            except Exception as e:\n",
    "                maf = 0.005\n",
    "                #raise ValueError(\"VCF line {}:{} does not have valid allele frequency field {}!\".\\\n",
    "                #                 format(self.vcf.GetChrom(), self.vcf.GetPosition(), self.af_info))\n",
    "            data.variants.append([self.vcf.GetChrom(), self.vcf.GetPosition(), self.name, maf])\n",
    "            # for each family assign member genotype if the site is non-trivial to the family\n",
    "            for k in data.families:\n",
    "                gs = self.vcf.GetGenotypes(data.famsampidx[k])\n",
    "                if len(set(''.join([x for x in gs if x != \"00\"]))) <= 1:\n",
    "                    # skip monomorphic gs\n",
    "                    continue\n",
    "                else:\n",
    "                    # this variant is found in the family\n",
    "                    data.famvaridx[k].append(varIdx)\n",
    "                    for person, g in zip(data.families[k], gs):\n",
    "                        data[person].append(g)\n",
    "            varIdx += 1\n",
    "        #\n",
    "        if varIdx == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            with env.variants_counter.get_lock():\n",
    "                env.variants_counter.value += varIdx\n",
    "            return 0\n",
    "\n",
    "\n",
    "    def getRegion(self, region):\n",
    "        self.chrom, self.startpos, self.endpos, self.name = region[:4]\n",
    "        self.startpos = int(self.startpos)\n",
    "        self.endpos = int(self.endpos) + 1\n",
    "        if self.chrom in ['X','23']:\n",
    "            if self.xchecker.check(self.startpos) or self.xchecker.check(self.endpos):\n",
    "                self.chrom = 'XY'\n",
    "        if self.chrom in ['Y','24']:\n",
    "            if self.ychecker.check(self.startpos) or self.ychecker.check(self.endpos):\n",
    "                self.chrom = 'XY'\n",
    "        if self.chr_prefix and not self.chrom.startswith(self.chr_prefix):\n",
    "            self.chrom = self.chr_prefix + self.chrom\n",
    "\n",
    "\n",
    "class MarkerMaker:\n",
    "    def __init__(self, wsize, maf_cutoff = None):\n",
    "        self.missings = (\"0\", \"0\")\n",
    "        self.gtconv = {'1':0, '2':1}\n",
    "        self.haplotyper = cstatgen.HaplotypingEngine(verbose = env.debug)\n",
    "        if wsize == 0 or wsize >= 1:\n",
    "            self.r2 = None\n",
    "        else:\n",
    "            self.r2 = wsize\n",
    "        self.coder = cstatgen.HaplotypeCoder(wsize)\n",
    "        self.maf_cutoff = maf_cutoff\n",
    "        self.rsq = 0.0\n",
    "\n",
    "    def apply(self, data):\n",
    "        # temp raw haplotype, maf and variant names data\n",
    "        haplotypes = OrderedDict()\n",
    "        mafs = {}\n",
    "        varnames = {}\n",
    "        #try:\n",
    "            # haplotyping plus collect found allele counts\n",
    "            # and computer founder MAFS\n",
    "        self.__Haplotype(data, haplotypes, mafs, varnames)\n",
    "        if len(varnames):\n",
    "            if not any ([len(varnames[x]) - 1 for x in varnames]):\n",
    "                # all families have only one variant\n",
    "                self.__AssignSNVHaplotypes(data, haplotypes, mafs, varnames)\n",
    "            else:\n",
    "                # calculate LD clusters using founder haplotypes\n",
    "                clusters = self.__ClusterByLD(data, haplotypes, varnames)\n",
    "                # recoding the genotype of the region\n",
    "                self.__CodeHaplotypes(data, haplotypes, mafs, varnames, clusters)\n",
    "        #except Exception as e:\n",
    "        #    return -1\n",
    "        self.__FormatHaplotypes(data)\n",
    "        return 0\n",
    "\n",
    "    def __Haplotype(self, data, haplotypes, mafs, varnames):\n",
    "        '''genetic haplotyping. haplotypes stores per family data'''\n",
    "        # FIXME: it is SWIG's (2.0.12) fault not to properly destroy the object \"Pedigree\" in \"Execute()\"\n",
    "        # So there is a memory leak here which I tried to partially handle on C++\n",
    "        #\n",
    "        # Per family haplotyping\n",
    "        #\n",
    "        self.markers = [\"V{}-{}\".format(idx, item[1]) for idx, item in enumerate(data.variants)]\n",
    "        for item in data.families:\n",
    "            varnames[item], positions, vcf_mafs = data.getFamVariants(item, style = \"map\")\n",
    "            if len(varnames[item]) == 0:\n",
    "                for person in data.families[item]:\n",
    "                    data[person] = self.missings\n",
    "                continue\n",
    "            if env.debug:\n",
    "                with env.lock:\n",
    "                    sys.stderr.write('\\n'.join(['\\t'.join(x) for x in data.getFamSamples(item)]) + '\\n\\n')\n",
    "            # haplotyping\n",
    "            with env.lock:\n",
    "                if not env.prephased:\n",
    "                    #with stdoutRedirect(to = env.tmp_log + str(os.getpid()) + '.log'):\n",
    "                    #    haplotypes[item] = self.haplotyper.Execute(data.chrom, varnames[item],\n",
    "                    #                                           sorted(positions), data.getFamSamples(item))[0]\n",
    "                    tmp_log_output=env.tmp_log + str(os.getpid()) + '.log'\n",
    "                    if isnotebook():\n",
    "                        haplotypes[item] = self.haplotyper.Execute(data.chrom, varnames[item], sorted(positions), \n",
    "                                                                       data.getFamSamples(item), self.rsq, tmp_log_output)[0]\n",
    "                    else:\n",
    "                        with stdoutRedirect(to = tmp_log_output + '.log'):\n",
    "                            haplotypes[item] = self.haplotyper.Execute(data.chrom, varnames[item], sorted(positions), \n",
    "                                                                       data.getFamSamples(item), self.rsq, tmp_log_output)[0]\n",
    "\n",
    "                else:\n",
    "                    haplotypes[item] = self.__PedToHaplotype(data.getFamSamples(item))\n",
    "            if len(haplotypes[item]) == 0:\n",
    "                # C++ haplotyping implementation failed\n",
    "                with env.chperror_counter.get_lock():\n",
    "                    env.chperror_counter.value += 1\n",
    "            # either use privided MAF or computer MAF\n",
    "            if all(vcf_mafs):\n",
    "                for idx, v in enumerate(varnames[item]):\n",
    "                    if v not in mafs:\n",
    "                        mafs[v] = vcf_mafs[idx]\n",
    "            else:\n",
    "                # count founder alleles\n",
    "                print('count founder alleles')\n",
    "                for hap in haplotypes[item]:\n",
    "                    if not data.tfam.is_founder(hap[1]):\n",
    "                        continue\n",
    "                    for idxv, v in enumerate(varnames[item]):\n",
    "                        if v not in mafs:\n",
    "                            # [#alt, #haplotypes]\n",
    "                            mafs[v] = [0, 0]\n",
    "                        gt = hap[2 + idxv][1] if hap[2 + idxv][0].isupper() else hap[2 + idxv][0]\n",
    "                        if not gt == \"?\":\n",
    "                            mafs[v][0] += self.gtconv[gt]\n",
    "                            mafs[v][1] += 1.0\n",
    "        #\n",
    "        # Compute founder MAFs\n",
    "        #\n",
    "        for v in mafs:\n",
    "            if type(mafs[v]) is not list:\n",
    "                continue\n",
    "            mafs[v] = mafs[v][0] / mafs[v][1] if mafs[v][1] > 0 else 0.0\n",
    "        if env.debug:\n",
    "            with env.lock:\n",
    "                print(\"variant mafs = \", mafs, \"\\n\", file = sys.stderr)\n",
    "        #\n",
    "        # Drop some variants if maf is greater than given threshold\n",
    "        #\n",
    "        if self.maf_cutoff is not None:\n",
    "            exclude_vars = []\n",
    "            for v in mafs.keys():\n",
    "                if mafs[v] > self.maf_cutoff:\n",
    "                    exclude_vars.append(v)\n",
    "            for i in haplotypes.keys():\n",
    "                haplotypes[i] = listit(haplotypes[i])\n",
    "                for j in range(len(haplotypes[i])):\n",
    "                    haplotypes[i][j] = haplotypes[i][j][:2] + \\\n",
    "                      [x for idx, x in enumerate(haplotypes[i][j][2:]) if varnames[i][idx] not in exclude_vars]\n",
    "                varnames[i] = [x for x in varnames[i] if x not in exclude_vars]\n",
    "                # handle trivial data\n",
    "                if len(varnames[i]) == 0:\n",
    "                    for person in data.families[i]:\n",
    "                        data[person] = self.missings\n",
    "                    del varnames[i]\n",
    "                    del haplotypes[i]\n",
    "            # count how many variants are removed\n",
    "            with env.commonvar_counter.get_lock():\n",
    "                env.commonvar_counter.value += len(exclude_vars)\n",
    "\n",
    "\n",
    "    def __ClusterByLD(self, data, haplotypes, varnames):\n",
    "        if self.r2 is None:\n",
    "            return None\n",
    "        # get founder haplotypes\n",
    "        founder_haplotypes = []\n",
    "        markers = sorted(set(itertools.chain(*varnames.values())), key = lambda x: int(x.split(\"-\")[0][1:]))\n",
    "        for item in haplotypes:\n",
    "            for ihap, hap in enumerate(haplotypes[item]):\n",
    "                if not data.tfam.is_founder(hap[1]):\n",
    "                    continue\n",
    "                gt = [hap[2 + varnames[item].index(v)] if v in varnames[item] else '?' for v in markers]\n",
    "                founder_haplotypes.append((\"{}-{}\".format(hap[1], ihap % 2), \"\".join([x[1] if x[0].isupper() else x[0] for x in gt])))\n",
    "        # calculate LD blocks, use r2 measure\n",
    "        ld = Align.create(founder_haplotypes).matrixLD(validCharacters=\"12\")[\"r2\"]\n",
    "        blocks = []\n",
    "        for j in ld:\n",
    "            block = [j]\n",
    "            for k in ld[j]:\n",
    "                if ld[j][k] > self.r2:\n",
    "                    block.append(k)\n",
    "            if len(block) > 1:\n",
    "                blocks.append(block)\n",
    "        # get LD clusters\n",
    "        clusters = [[markers[idx] for idx in item] for item in list(connected_components(blocks))]\n",
    "        if env.debug:\n",
    "            with env.lock:\n",
    "                print(\"LD blocks: \", blocks, file = sys.stderr)\n",
    "                print(\"LD clusters: \", clusters, file = sys.stderr)\n",
    "        return clusters\n",
    "\n",
    "\n",
    "    def __CodeHaplotypes(self, data, haplotypes, mafs, varnames, clusters):\n",
    "        # apply CHP coding\n",
    "        if clusters is not None:\n",
    "            clusters_idx = [[[varnames[item].index(x) for x in y] for y in clusters] for item in haplotypes]\n",
    "        else:\n",
    "            clusters_idx = [[[]] for item in haplotypes]\n",
    "        self.coder.Execute(haplotypes.values(), [[mafs[v] for v in varnames[item]] for item in haplotypes], clusters_idx)\n",
    "        if env.debug:\n",
    "            with env.lock:\n",
    "                if clusters:\n",
    "                    print(\"Family LD clusters: \", clusters_idx, \"\\n\", file = sys.stderr)\n",
    "                self.coder.Print()\n",
    "        # line: [fid, sid, hap1, hap2]\n",
    "        for line in self.coder.GetHaplotypes():\n",
    "            if not line[1] in data:\n",
    "                # this sample is not in VCF file. Every variant site should be missing\n",
    "                # they have to be skipped for now\n",
    "                continue\n",
    "            data[line[1]] = (line[2].split(','), line[3].split(','))\n",
    "            if len(data[line[1]][0]) > data.superMarkerCount:\n",
    "                data.superMarkerCount = len(data[line[1]][0])\n",
    "        # get MAF\n",
    "        for item in haplotypes:\n",
    "            data.maf[item] = self.coder.GetAlleleFrequencies(item)\n",
    "            data.maf[item] = tuple(tuple(np.array(v) / np.sum(v)) if np.sum(v) else v\n",
    "                              for v in data.maf[item])\n",
    "        if env.debug:\n",
    "            with env.lock:\n",
    "                print(\"marker freqs = \", data.maf, \"\\n\", file = sys.stderr)\n",
    "\n",
    "\n",
    "    def __AssignSNVHaplotypes(self, data, haplotypes, mafs, varnames):\n",
    "        for item in haplotypes:\n",
    "            # each person's haplotype\n",
    "            token = ''\n",
    "            for idx, line in enumerate(haplotypes[item]):\n",
    "                if not idx % 2:\n",
    "                    token = line[2][1] if line[2][0].isupper() else line[2][0]\n",
    "                else:\n",
    "                    data[line[1]] = (token, line[2][1] if line[2][0].isupper() else line[2][0])\n",
    "            # get maf\n",
    "            data.maf[item] = [(1 - mafs[varnames[item][0]], mafs[varnames[item][0]])]\n",
    "            data.maf[item] = tuple(tuple(np.array(v) / np.sum(v)) if np.sum(v) else v\n",
    "                              for v in data.maf[item])\n",
    "        if env.debug:\n",
    "            with env.lock:\n",
    "                print(\"marker freqs = \", data.maf, \"\\n\", file = sys.stderr)\n",
    "\n",
    "\n",
    "    def __FormatHaplotypes(self, data):\n",
    "        # Reformat sample genotypes\n",
    "        for person in data:\n",
    "            if type(data[person]) is not tuple:\n",
    "                data[person] = self.missings\n",
    "                continue\n",
    "            diff = data.superMarkerCount - len(data[person][0])\n",
    "            data[person] = zip(*data[person])\n",
    "            if diff > 0:\n",
    "                data[person].extend([self.missings] * diff)\n",
    "\n",
    "    def __PedToHaplotype(self, ped):\n",
    "        '''convert prephased ped format to haplotype format.\n",
    "        Input: e.g. [['13346', '5888', '0', '0', '1', '11', '11', '11'], ['13346', '5856', '0', '0', '2', '12', '12', '12'], ['13346', '5920', '5888', '5856', '1', '12', '12', '12'], ['13346', '6589', '5888', '5856', '1', '11', '11', '11']]\n",
    "        Output: e.g. (('13346', '5856', '1:', '1:', '1:'), ('13346', '5856', '2:', '2:', '2:'), ('13346', '5888', '1:', '1:', '1:'), ('13346', '5888', '1:', '1:', '1:'), ('13346', '6589', '1:', '1|', '1|'), ('13346', '6589', '1:', '1|', '1|'), ('13346', '5920', '2:', '2|', '2|'), ('13346', '5920', '1:', '1|', '1|'))\n",
    "        '''\n",
    "        haps = []\n",
    "        for item in ped:\n",
    "            entry = [item[0], item[1]] + [x[0] + ':' if x[0] != '0' else '?:' for x in item[5:]]\n",
    "            haps.append(tuple(entry))\n",
    "            entry = [item[0], item[1]] + [x[1] + ':' if x[1] != '0' else '?:' for x in item[5:]]\n",
    "            haps.append(tuple(entry))\n",
    "        return tuple(haps)\n",
    "\n",
    "\n",
    "class LinkageWriter:\n",
    "    def __init__(self, num_missing_append = 0):\n",
    "        self.chrom = self.prev_chrom = self.name = self.distance = self.distance_avg = self.distance_m = self.distance_f = None\n",
    "        self.reset()\n",
    "        self.missings = [\"0\", \"0\"]\n",
    "        self.num_missing = num_missing_append\n",
    "\n",
    "    def apply(self, data):\n",
    "        if self.chrom != self.prev_chrom:\n",
    "            if self.prev_chrom is None:\n",
    "                self.prev_chrom = self.chrom\n",
    "            else:\n",
    "                # new chrom entered,\n",
    "                # commit whatever is in buffer before accepting new data\n",
    "                self.commit()\n",
    "        # write tped output\n",
    "        position = str(data.getMidPosition())\n",
    "        if data.superMarkerCount <= 1:\n",
    "            # genotypes\n",
    "            gs = [data[s][0] for s in data.samples]\n",
    "            if len(set(gs)) == 1:\n",
    "                # everyone's genotype is the same (most likely missing or monomorphic)\n",
    "                return 2\n",
    "            self.tped += env.delimiter.join([self.chrom, self.name, self.distance, position] + \\\n",
    "                list(itertools.chain(*gs)) + self.missings*self.num_missing) + \"\\n\"\n",
    "            # freqs\n",
    "            for k in data.maf:\n",
    "                self.freq += env.delimiter.join([k, self.name] + map(str, data.maf[k][0])) + \"\\n\"\n",
    "        else:\n",
    "            # have to expand each region into mutiple chunks to account for different recomb points\n",
    "            gs = zip(*[data[s] for s in data.samples])\n",
    "            # sub-chunk id\n",
    "            cid = 0\n",
    "            skipped_chunk = []\n",
    "            for idx, g in enumerate(gs):\n",
    "                if len(set(g)) == 1:\n",
    "                    skipped_chunk.append(idx)\n",
    "                    continue\n",
    "                cid += 1\n",
    "                self.tped += \\\n",
    "                  env.delimiter.join([self.chrom, '{}[{}]'.format(self.name, cid), self.distance, position] + \\\n",
    "                  list(itertools.chain(*g)) + self.missings*self.num_missing) + \"\\n\"\n",
    "            if cid == 0:\n",
    "                # everyone's genotype is the same (most likely missing or monomorphic)\n",
    "                return 2\n",
    "            # freqs\n",
    "            for k in data.maf:\n",
    "                cid = 0\n",
    "                for idx in range(data.superMarkerCount):\n",
    "                    if idx in skipped_chunk:\n",
    "                        continue\n",
    "                    if idx >= len(data.maf[k]):\n",
    "                        break\n",
    "                    cid += 1\n",
    "                    self.freq += env.delimiter.join([k, '{}[{}]'.format(self.name, cid)] + \\\n",
    "                                                    map(str, data.maf[k][idx])) + \"\\n\"\n",
    "        if self.counter < env.batch:\n",
    "            self.counter += data.superMarkerCount\n",
    "        else:\n",
    "            self.commit()\n",
    "        return 0\n",
    "\n",
    "    def commit(self):\n",
    "        if self.tped:\n",
    "            with env.lock:\n",
    "                with open(os.path.join(env.tmp_cache, '{}.chr{}.tped'.format(env.output, self.prev_chrom)),\n",
    "                          'a') as f:\n",
    "                    f.write(self.tped)\n",
    "        if self.freq:\n",
    "            with env.lock:\n",
    "                with open(os.path.join(env.tmp_cache, '{}.chr{}.freq'.format(env.output, self.prev_chrom)),\n",
    "                          'a') as f:\n",
    "                    f.write(self.freq)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.tped = ''\n",
    "        self.freq = ''\n",
    "        self.counter = 0\n",
    "        self.prev_chrom = self.chrom\n",
    "\n",
    "    def getRegion(self, region):\n",
    "        self.chrom = region[0]\n",
    "        self.name, self.distance_avg, self.distance_m, self.distance_f = region[3:]\n",
    "        self.distance = \";\".join([self.distance_avg, self.distance_m, self.distance_f])\n",
    "\n",
    "\n",
    "class EncoderWorker(Process):\n",
    "    def __init__(self, queue, length, data, extractor, coder, writer):\n",
    "        Process.__init__(self)\n",
    "        self.queue = queue\n",
    "        self.numGrps = float(length)\n",
    "        self.data = data\n",
    "        self.extractor = extractor\n",
    "        self.maker = coder\n",
    "        self.writer = writer\n",
    "\n",
    "    def report(self):\n",
    "        env.log('{:,d} units processed {{{:.2%}}} ...'.\\\n",
    "                format(env.success_counter.value, env.total_counter.value / self.numGrps), flush = True)\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            try:\n",
    "                region = self.queue.get()\n",
    "                if region is None:\n",
    "                    self.writer.commit()\n",
    "                    self.report()\n",
    "                    # total mendelian errors found\n",
    "                    with env.mendelerror_counter.get_lock():\n",
    "                        env.mendelerror_counter.value += self.maker.haplotyper.CountMendelianErrors()\n",
    "                    # total recombination events found\n",
    "                    with env.recomb_counter.get_lock():\n",
    "                        env.recomb_counter.value += self.maker.coder.CountRecombinations()\n",
    "                    break\n",
    "                else:\n",
    "                    with env.total_counter.get_lock():\n",
    "                        env.total_counter.value += 1\n",
    "                    self.extractor.getRegion(region)\n",
    "                    self.writer.getRegion(region)\n",
    "                    isSuccess = True\n",
    "                    for m in [self.extractor, self.maker, self.writer]:\n",
    "                        status = m.apply(self.data)\n",
    "                        if status == -1:\n",
    "                            with env.chperror_counter.get_lock():\n",
    "                                # previous module failed\n",
    "                                env.chperror_counter.value += 1\n",
    "                        if status == 1:\n",
    "                            with env.null_counter.get_lock():\n",
    "                                env.null_counter.value += 1\n",
    "                        if status == 2:\n",
    "                            with env.trivial_counter.get_lock():\n",
    "                                env.trivial_counter.value += 1\n",
    "                        if status != 0:\n",
    "                            isSuccess = False\n",
    "                            break\n",
    "                    if isSuccess:\n",
    "                        with env.success_counter.get_lock():\n",
    "                            env.success_counter.value += 1\n",
    "                    if env.total_counter.value % (env.batch * env.jobs) == 0:\n",
    "                        self.report()\n",
    "            except KeyboardInterrupt:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of SEQLinkage.Utils failed: Traceback (most recent call last):\n",
      "  File \"/home/yh3455/miniconda3/envs/seqlink_100/lib/python2.7/site-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"SEQLinkage/Utils.py\", line 151\n",
      "    global env = Environment()\n",
      "               ^\n",
      "SyntaxError: invalid syntax\n",
      "]\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-30-158132e2d088>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-30-158132e2d088>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    global env = Environment()\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "global env = Environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;40;32mMESSAGE: Binary trait detected in [/mnt/mfs/statgen/yin/Github/linkage/sample_i/rare_positions/sample_i_coding.hg38_multianno.fam]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkParams(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AF'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.no_save:\n",
    "    cache = NoCache()\n",
    "else:\n",
    "    cache = Cache(env.cache_dir, env.output, vars(args))\n",
    "cache.setID('vcf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;40;32mMESSAGE: 166 samples found in [/mnt/mfs/statgen/yin/Github/linkage/sample_i/rare_positions/sample_i_coding.hg38_multianno.vcf.gz]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: write encoded data to TPED format\n",
    "if not args.vanilla and cache.check():\n",
    "    env.log('Loading regional marker data from archive ...')\n",
    "    cache.load(target_dir = env.tmp_dir, names = ['CACHE'])\n",
    "    env.success_counter.value = sum(map(fileLinesCount, glob.glob('{}/*.tped'.format(env.tmp_cache))))\n",
    "    env.batch = 10\n",
    "else:\n",
    "    # load VCF file header\n",
    "    data = RData(args.vcf, args.tfam)\n",
    "    vs = data.vs\n",
    "    samples_vcf = data.samples_vcf\n",
    "\n",
    "if len(samples_vcf) == 0:\n",
    "    env.error(\"Fail to extract samples from [{}]\".format(args.vcf), exit = True)\n",
    "env.log('{:,d} samples found in [{}]'.format(len(samples_vcf), args.vcf))\n",
    "samples_not_vcf = data.samples_not_vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 166)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples_not_vcf),len(samples_vcf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.tfam.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(data.families) == 0:\n",
    "    env.error('No valid family to process. ' \\\n",
    "              'Families have to be at least trio with at least one member in VCF file.', exit = True)\n",
    "if len(data.samples) == 0:\n",
    "    env.error('No valid sample to process. ' \\\n",
    "              'Samples have to be in families, and present in both TFAM and VCF files.', exit = True)\n",
    "rewriteFamfile(os.path.join(env.tmp_cache, '{}.tfam'.format(env.output)),\n",
    "               data.tfam.samples, list(data.samples.keys()) + samples_not_vcf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;40;32mMESSAGE: 35 families with a total of 166 samples will be scanned for 28,325 pre-defined units\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if args.single_markers:\n",
    "    regions=[]\n",
    "    for x in vs.GetGenomeCoordinates():\n",
    "        region_info = (x[0], x[1], x[1], \"{}:{}\".format(x[0], x[1]), '.', '.', '.')\n",
    "        if region_info not in regions:\n",
    "            regions.append(region_info)\n",
    "    args.blueprint = None\n",
    "else:\n",
    "    # load blueprint\n",
    "    try:\n",
    "        with open(args.blueprint, 'r') as f:\n",
    "            regions = [x.strip().split() for x in f.readlines()]\n",
    "    except IOError:\n",
    "        env.error(\"Cannot load regional marker blueprint [{}]. \".format(args.blueprint), exit = True)\n",
    "env.log('{:,d} families with a total of {:,d} samples will be scanned for {:,d} pre-defined units'.\\\n",
    "        format(len(data.families), len(data.samples), len(regions)))\n",
    "env.jobs = max(min(args.jobs, len(regions)), 1)\n",
    "regions.extend([None] * env.jobs)\n",
    "queue = [] if env.jobs == 1 else Queue()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faulthandler.enable(file=open(env.tmp_log + '.SEGV', 'w'))\n",
    "for i in regions:\n",
    "    if isinstance(queue, list):\n",
    "        queue.append(i)\n",
    "    else:\n",
    "        queue.put(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_by_fam_flag = False\n",
    "if not args.freq_by_fam is None:\n",
    "    freq_by_fam_flag = True\n",
    "    with open(args.freq_by_fam) as freq_fh:\n",
    "        for freq_line in freq_fh:\n",
    "            tmp_eles=freq_line.split()   #Fam and Population\n",
    "            data.freq_by_fam[tmp_eles[0]]=tmp_eles[1]\n",
    "    data.freq=sorted(list(set(data.freq_by_fam.values())))\n",
    "else:\n",
    "    data.freq=args.freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jobs = [EncoderWorker(\n",
    "#             queue, len(regions), deepcopy(data),\n",
    "#             RegionExtractor(args.vcf, chr_prefix = args.chr_prefix, allele_freq_info = data.freq, include_vars_file=args.include_vars),\n",
    "#             MarkerMaker(args.bin, maf_cutoff = args.maf_cutoff,single_markers=args.single_markers,\n",
    "#                         recomb_max=args.recomb_max,af_info=data.freq,freq_by_fam=freq_by_fam_flag,rsq=args.rsq,\n",
    "#                         mle=args.mle, rvhaplo=args.rvhaplo, recomb_perfam=not args.recomb_cross_fam),\n",
    "#             LinkageWriter(len(samples_not_vcf))\n",
    "#             ) for i in range(env.jobs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg = [['1',\n",
    "  '176432306',\n",
    "  '176811970',\n",
    "  'PAPPA2',\n",
    "  '186.278964324',\n",
    "  '238.991541401',\n",
    "  '136.402021932'],\n",
    " ['16',\n",
    "  '89984286',\n",
    "  '89987385',\n",
    "  'MC1R',\n",
    "  '133.689089888',\n",
    "  '159.050776809',\n",
    "  '111.195245154'],\n",
    " ['19', '281039', '291435', 'PPAP2C', '0.0', '0.0', '0.0'],\n",
    " ['19', '281039', '291435', 'PPAP2C', '0.0', '0.0', '0.0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = regions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'region' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a7b6f383b3da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mregion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'region' is not defined"
     ]
    }
   ],
   "source": [
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.freq = args.freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor =RegionExtractor(args.vcf, chr_prefix = args.chr_prefix, allele_freq_info = args.freq)\n",
    "maker =            MarkerMaker(args.bin, maf_cutoff = args.maf_cutoff)\n",
    "writer =             LinkageWriter(len(samples_not_vcf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 0 ['1', '11868', '14362', 'LOC102725121@1', '9.177127474362311e-07', '1.1657192989882668e-06', '6.814189157634088e-07']\n",
      "1000 3 81 ['1', '61742479', '62163915', 'PATJ', '94.08912254193991', '118.69606841169087', '71.63174343660968']\n",
      "2000 3 7 ['1', '162790701', '162812818', 'HSD17B7', '168.91829636079495', '218.49549236802426', '122.20452659549645']\n",
      "3000 1 0 ['2', '27212359', '27217182', 'ATRAID', '48.19894998090023', '54.585310839481366', '42.49830744092714']\n",
      "4000 1 0 ['2', '168920780', '169031324', 'ABCB11', '174.1718395156357', '222.22338807109554', '127.6844708222256']\n",
      "5000 1 0 ['3', '47413680', '47475941', 'SCAP', '68.99620573382401', '81.61445932132666', '57.9855449134992']\n",
      "6000 1 0 ['3', '184230352', '184242329', 'VWA5B2', '192.1275334861548', '247.45672160907745', '139.2580545053618']\n",
      "7000 1 0 ['4', '138164096', '138242349', 'SLC7A11', '140.35374100483878', '186.05001188945482', '97.20783019850205']\n",
      "8000 1 0 ['5', '132190146', '132227853', 'P4HA2', '138.85770413097532', '173.97439801388742', '105.14936291323124']\n",
      "9000 1 0 ['6', '31615233', '31617015', 'AIF1', '53.57404706283638', '56.98460061173304', '51.484032621407884']\n",
      "10000 1 0 ['6', '167155246', '167157980', 'GPR31', '187.8212404552102', '243.3868606324144', '134.9096549771634']\n",
      "11000 1 0 ['7', '113080412', '113087778', 'GPR85', '121.94724950804803', '163.03027920118186', '82.53468270653605']\n",
      "12000 1 0 ['8', '81478418', '81483233', 'FABP4', '95.00012872224329', '125.79097683477232', '65.891333925847']\n",
      "13000 1 0 ['9', '95085207', '95085304', 'MIR23B', '100.77698198363763', '124.30346632003499', '78.55681544180344']\n",
      "14000 1 0 ['10', '58325628', '58329679', 'LOC112268068', '74.88956742062021', '94.58971593360282', '57.27601225803206']\n",
      "15000 3 45 ['11', '12286899', '12362138', 'MICALCL', '25.200421811362663', '19.780182273947002', '30.47861020169645']\n",
      "16000 3 19 ['11', '101890673', '101916522', 'ANGPTL5', '109.59131065684834', '139.3668510208155', '80.95094926518013']\n",
      "17000 3 27 ['12', '54395260', '54419266', 'ITGA5', '72.35531487721212', '85.30635133411202', '60.57391809500565']\n",
      "18000 1 0 ['13', '46455203', '46466776', 'LINC01198', '50.167966040012665', '57.175574695883995', '43.125175052271366']\n",
      "19000 1 0 ['14', '86905777', '86922755', 'LINC01148', '84.32939464542692', '103.04064213211844', '66.78106340164302']\n",
      "20000 3 12 ['15', '73873563', '73889214', 'TBC1D21', '77.64806530153696', '97.22184379127768', '58.61042722911022']\n",
      "21000 1 0 ['16', '33495800', '33496235', 'LOC390705@2', '59.597025459459736', '64.51252841288981', '55.757286567026384']\n",
      "22000 1 0 ['17', '28360653', '28360734', 'MIR4723', '53.29906731104194', '56.137810791384275', '49.8686532838509']\n",
      "23000 3 3 ['17', '79823451', '79827704', 'LINC01977', '129.41707434188964', '161.0935891249485', '98.15793907670367']\n",
      "24000 3 16 ['19', '12953118', '12957223', 'GADD45GIP1', '33.20123193440195', '31.021078181726253', '35.714493705610096']\n",
      "25000 3 23 ['19', '51964339', '51986840', 'ZNF350', '89.6950481101567', '113.9831205106624', '66.44497607852102']\n",
      "26000 1 0 ['20', '62143768', '62182514', 'SS18L1', '110.30700665717275', '124.48353231287628', '98.56286538747486']\n",
      "27000 1 0 ['22', '43110749', '43129712', 'BIK', '54.13818873093194', '71.04904715507725', '37.884233842626216']\n",
      "28000 1 0 ['X', '130494940', '130497447', 'DENND10P1', 'NA', '146.88487435565696', 'NA']\n"
     ]
    }
   ],
   "source": [
    "for j, region in enumerate(regions[:-20]):\n",
    "    i = 0\n",
    "    #for region in rg:\n",
    "    extractor.getRegion(region)\n",
    "    #maker.getRegion(region)\n",
    "    writer.getRegion(region)\n",
    "    isSuccess = True\n",
    "    for m in [extractor, maker, writer]:\n",
    "        status = m.apply(data)\n",
    "        i+=1\n",
    "        if status == -1:\n",
    "            with env.chperror_counter.get_lock():\n",
    "                # previous module failed\n",
    "                env.chperror_counter.value += 1\n",
    "        if status == 1:\n",
    "            with env.null_counter.get_lock():\n",
    "                env.null_counter.value += 1\n",
    "        if status == 2:\n",
    "            with env.trivial_counter.get_lock():\n",
    "                env.trivial_counter.value += 1\n",
    "        if status != 0:\n",
    "            isSuccess = False\n",
    "            break\n",
    "    if isSuccess:\n",
    "        with env.success_counter.get_lock():\n",
    "            env.success_counter.value += 1\n",
    "    if j%1000==0:\n",
    "        print(j,i,len(data.variants),region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One region test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = ['1',\n",
    " '12879211',\n",
    " '12886201',\n",
    " 'PRAMEF4',\n",
    " '28.75680822077905',\n",
    " '29.7968659448725',\n",
    " '29.5317245115793']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor.getRegion(region)\n",
    "writer.getRegion(region)  \n",
    "extractor.apply(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker.apply(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all([0.0865, 0.0346, 0.005, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all([0.0009, 0.005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haplotypes = OrderedDict()\n",
    "mafs = {}   ##Per fam per variant\n",
    "uniq_vars = []\n",
    "exclude_vars = []\n",
    "varnames = {}\n",
    "recombPos = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.markers = [\"V{}-{}\".format(idx, item[1]) for idx, item in enumerate(data.variants)]\n",
    "for item in data.families:\n",
    "    varnames[item], positions, vcf_mafs = data.getFamVariants(item, style = \"map\")\n",
    "    if len(varnames[item]) == 0:\n",
    "        for person in data.families[item]:\n",
    "            data[person] = self.missings\n",
    "        continue\n",
    "    if env.debug:\n",
    "        with env.lock:\n",
    "            sys.stderr.write('\\n'.join(['\\t'.join(x) for x in data.getFamSamples(item)]) + '\\n\\n')\n",
    "    # haplotyping\n",
    "    with env.lock:\n",
    "        if not env.prephased:\n",
    "            #with stdoutRedirect(to = env.tmp_log + str(os.getpid()) + '.log'):\n",
    "            #    haplotypes[item] = self.haplotyper.Execute(data.chrom, varnames[item],\n",
    "            #                                           sorted(positions), data.getFamSamples(item))[0]\n",
    "            tmp_log_output=env.tmp_log + str(os.getpid())\n",
    "            #with stdoutRedirect(to = tmp_log_output + '.log'):\n",
    "            haplotypes[item] = self.haplotyper.Execute(data.chrom, varnames[item], sorted(positions),\n",
    "                                                           data.getFamSamples(item), self.rsq, tmp_log_output)[0]\n",
    "\n",
    "        else:\n",
    "            haplotypes[item] = self.__PedToHaplotype(data.getFamSamples(item))\n",
    "    if len(haplotypes[item]) == 0:\n",
    "        # C++ haplotyping implementation failed\n",
    "        with env.chperror_counter.get_lock():\n",
    "            env.chperror_counter.value += 1\n",
    "    # either use privided MAF or computer MAF\n",
    "    if all(vcf_mafs):\n",
    "        for idx, v in enumerate(varnames[item]):\n",
    "            if v not in mafs:\n",
    "                mafs[v] = vcf_mafs[idx]\n",
    "    else:\n",
    "        # count founder alleles\n",
    "        for hap in haplotypes[item]:\n",
    "            if not data.tfam.is_founder(hap[1]):\n",
    "                continue\n",
    "            for idxv, v in enumerate(varnames[item]):\n",
    "                if v not in mafs:\n",
    "                    # [#alt, #haplotypes]\n",
    "                    mafs[v] = [0, 0]\n",
    "                gt = hap[2 + idxv][1] if hap[2 + idxv][0].isupper() else hap[2 + idxv][0]\n",
    "                if not gt == \"?\":\n",
    "                    mafs[v][0] += self.gtconv[gt]\n",
    "                    mafs[v][1] += 1.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker._MarkerMaker__Haplotype(data, haplotypes, mafs, varnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    if env.triallelic_counter.value:\n",
    "        env.log('{:,d} tri-allelic loci were ignored'.format(env.triallelic_counter.value))\n",
    "    if env.commonvar_counter.value:\n",
    "        env.log('{:,d} variants ignored due to having MAF > {} and other specified constraints'.\\\n",
    "                format(env.commonvar_counter.value, args.maf_cutoff))\n",
    "    if env.null_counter.value:\n",
    "        env.log('{:,d} units ignored due to absence in VCF file'.format(env.null_counter.value))\n",
    "    if env.trivial_counter.value:\n",
    "        env.log('{:,d} units ignored due to absence of variation in samples'.format(env.trivial_counter.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fatal_errors = 0\n",
    "try:\n",
    "    # Error msg from C++ extension\n",
    "    os.system(\"cat {}/*.* > {}\".format(env.tmp_dir, env.tmp_log))\n",
    "    fatal_errors = wordCount(env.tmp_log)['fatal']\n",
    "except KeyError:\n",
    "    pass\n",
    "if env.chperror_counter.value:\n",
    "    env.error(\"{:,d} regional markers failed to be generated due to haplotyping failures!\".\\\n",
    "              format(env.chperror_counter.value))\n",
    "if fatal_errors:\n",
    "    env.error(\"{:,d} or more regional markers failed to be generated due to runtime errors!\".\\\n",
    "              format(fatal_errors))\n",
    "env.log('Archiving regional marker data to directory [{}]'.format(env.cache_dir))\n",
    "cache.write(arcroot = 'CACHE', source_dir = env.tmp_cache)\n",
    "env.jobs = args.jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/1967077.1.plot.q/SEQLinkage_tmp_6rffCe/CACHE'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.tmp_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(env.tmp_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: write to PLINK or mega2 format\n",
    "tpeds = [os.path.join(env.tmp_cache, item) for item in os.listdir(env.tmp_cache) if item.startswith(env.output) and item.endswith('.tped')]\n",
    "for fmt in args.format:\n",
    "    cache.setID(fmt)\n",
    "    if not args.vanilla and cache.check():\n",
    "        env.log('Loading {} data from archive ...'.format(fmt.upper()))\n",
    "        cache.load(target_dir = env.tmp_dir, names = [fmt.upper()])\n",
    "    else:\n",
    "        env.log('{:,d} units will be converted to {} format'.format(env.success_counter.value, fmt.upper()))\n",
    "        env.format_counter.value = 0\n",
    "        format(tpeds, os.path.join(env.tmp_cache, \"{}.tfam\".format(env.output)),\n",
    "               args.prevalence, args.wild_pen, args.muta_pen, fmt,\n",
    "               args.inherit_mode, args.theta_max, args.theta_inc)\n",
    "        env.log('{:,d} units successfully converted to {} format\\n'.\\\n",
    "                format(env.format_counter.value, fmt.upper()), flush = True)\n",
    "        if env.skipped_counter.value:\n",
    "            # FIXME: perhaps we need to rephrase this message?\n",
    "            env.log('{} region - family pairs skipped'.\\\n",
    "                    format(env.skipped_counter.value))\n",
    "        env.log('Archiving {} format to directory [{}]'.format(fmt.upper(), env.cache_dir))\n",
    "        cache.write(arcroot = fmt.upper(),\n",
    "                    source_dir = os.path.join(env.tmp_dir, fmt.upper()), mode = 'a')\n",
    "mkpath(env.outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;40;32mMESSAGE: Running linkage analysis ...\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Environment instance has no attribute 'jobs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-298360bf62ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running linkage analysis ...'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mrun_linkage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblueprint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta_inc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_limit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Linkage analysis succesfully performed for {:,d} units\\n'\u001b[0m\u001b[0;34m.\u001b[0m                \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakeped_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/mfs/statgen/yin/Github/linkage/SEQpy2/SEQLinkage/Runner.pyc\u001b[0m in \u001b[0;36mrun_linkage\u001b[0;34m(blueprint, theta_inc, theta_max, to_plot)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LinkageRuntimeError.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mruntime_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mworkdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/LINKAGE/{}.chr*'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0mparmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlinkage_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblueprint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_inc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mruntime_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_plot\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mworkdirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlinkage_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblueprint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_inc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Environment instance has no attribute 'jobs'"
     ]
    }
   ],
   "source": [
    "if args.run_linkage:\n",
    "    cache.setID('analysis')\n",
    "    if not args.vanilla and cache.check():\n",
    "        env.log('Loading linkage analysis result from archive ...'.format(fmt.upper()))\n",
    "        cache.load(target_dir = env.output, names = ['heatmap'])\n",
    "    else:\n",
    "        env.log('Running linkage analysis ...'.format(fmt.upper()))\n",
    "        run_linkage(args.blueprint, args.theta_inc, args.theta_max, args.output_limit)\n",
    "        env.log('Linkage analysis succesfully performed for {:,d} units\\n'.\\\n",
    "                format(env.run_counter.value, fmt.upper()), flush = True)\n",
    "        if env.makeped_counter.value:\n",
    "            env.log('{} \"makeped\" runtime errors occurred'.format(env.makeped_counter.value))\n",
    "        if env.pedcheck_counter.value:\n",
    "            env.log('{} \"pedcheck\" runtime errors occurred'.format(env.pedcheck_counter.value))\n",
    "        if env.unknown_counter.value:\n",
    "            env.log('{} \"unknown\" runtime errors occurred'.format(env.unknown_counter.value))\n",
    "        if env.mlink_counter.value:\n",
    "            env.log('{} \"mlink\" runtime errors occurred'.format(env.mlink_counter.value))\n",
    "        cache.write(arcroot = 'heatmap', source_dir = os.path.join(env.output, 'heatmap'), mode = 'a')\n",
    "    html(args.theta_inc, args.theta_max, args.output_limit)\n",
    "else:\n",
    "    env.log('Saving data to [{}]'.format(os.path.abspath(env.output)))\n",
    "    cache.load(target_dir = env.output, names = [fmt.upper() for fmt in args.format])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mrun_linkage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblueprint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_inc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_plot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m      /mnt/mfs/statgen/yin/Github/linkage/SEQpy2/SEQLinkage/Runner.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?run_linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.run_linkage = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = ['1', '69090', '70008', 'OR4F5', '4.866641545668504e-06', '6.181823219621424e-06', '3.6135725636621673e-06']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor.getRegion(region)\n",
    "maker.getRegion(region)\n",
    "writer.getRegion(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haplotypes = OrderedDict()\n",
    "mafs = {}   ##Per fam per variant\n",
    "uniq_vars = []\n",
    "exclude_vars = []\n",
    "varnames = {}\n",
    "recombPos = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor.apply(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker._MarkerMaker__Haplotype(data, haplotypes, mafs, varnames,recombPos,uniq_vars,exclude_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haplotypes['668']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker._MarkerMaker__ClusterByLD(data, haplotypes, varnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker._MarkerMaker__CodeHaplotypes(data, haplotypes, mafs, varnames, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = []\n",
    "if clusters is not None:\n",
    "    clusters_idx = [[[varnames[item].index(x) for x in y] for y in clusters] for item in haplotypes]\n",
    "else:\n",
    "    clusters_idx = [[[]] for item in haplotypes]\n",
    "maker.coder.Execute(haplotypes.values(), [[mafs[v] for v in varnames[item]] for item in haplotypes], clusters_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker.coder.Print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker.ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[mafs[item][v] for v in varnames[item]] for item in haplotypes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames['668']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mafs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test clusterbyld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker.apply(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haplotypes = OrderedDict()\n",
    "mafs = {}\n",
    "varnames = {}\n",
    "maker._MarkerMaker__Haplotype(data, haplotypes, mafs, varnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = [\"V{}-{}\".format(idx, item[1]) for idx, item in enumerate(data.variants)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = list(data.families.keys())[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item ='1036'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames[item], positions, vcf_mafs = data.getFamVariants(item, style = \"map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker.haplotyper.Execute(data.chrom, varnames[item], sorted(positions), data.getFamSamples(item))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recombPos={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames[item], positions, vcf_mafs = data.getFamVariants(item, style = \"map\")\n",
    "recombPos[item]={}\n",
    "var_for_haplotype=[]\n",
    "positions_for_haplotype=[]\n",
    "output_sample=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.gnomAD_estimate.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_for_haplotype=varnames[item]\n",
    "positions_for_haplotype=positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "famid =item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_names = []\n",
    "S_no_parents = filter(lambda x: True if data.tfam.is_founder(x) else False, data.tfam.families[famid])\n",
    "graph = data.tfam.graph[famid].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(S_no_parents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tfam.families[famid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(S_no_parents):\n",
    "    n = S_no_parents.pop()\n",
    "    sorted_names.append(n)\n",
    "    if n not in graph:\n",
    "        continue\n",
    "    offsprings = graph.pop(n)\n",
    "    for m in offsprings:\n",
    "        father, mother = data.tfam.get_parents(m)\n",
    "        if father not in graph and mother not in graph:\n",
    "            S_no_parents.append(m)\n",
    "if graph:\n",
    "    raise ValueError(\"There is a loop in the pedigree: {}\\n\".format(' '.join(graph.keys())))\n",
    "else:\n",
    "    return sorted_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tfam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect sample+genotypes\n",
    "for person in data.tfam.sort_family(item):\n",
    "    output_sample.append([])\n",
    "    last_ele=len(output_sample)-1\n",
    "    output_sample[last_ele] = data.tfam.samples[person][:-1]\n",
    "    if person in data.samples:\n",
    "        for marker in var_for_haplotype:\n",
    "            idx=int(marker.split('-')[0][1:])\n",
    "            output_sample[last_ele].append(data.genotype_all[person][idx])\n",
    "    else:\n",
    "        output_sample[last_ele].extend([\"00\"] * len(var_for_haplotype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.tfam.sort_family(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(output_sample[0][5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.tmp_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haplotypes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_log_output=env.tmp_log + str(os.getpid())\n",
    "haplotypes[item] = maker.haplotyper.Execute(data.chrom, var_for_haplotype, positions_for_haplotype, output_sample, maker.rsq, tmp_log_output)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_for_haplotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions_for_haplotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haplotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haplotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_for_haplotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions_for_haplotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker.rsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haplotypes['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for item in haplotypes:\n",
    "for hap_idx,haploid in enumerate(haplotypes[item]):\n",
    "    for vidx,var in enumerate(haploid[2:]):\n",
    "        if not var.endswith(':') and not var.endswith('|') and vidx!=0:\n",
    "            postvar_name=varnames[item][vidx]\n",
    "            prevar_name=varnames[item][vidx-1]\n",
    "            recomb_pair = (prevar_name,postvar_name)\n",
    "            print('run this')\n",
    "            try:\n",
    "                recombPos[item][recomb_pair].append(hap_idx)\n",
    "            except:\n",
    "                recombPos[item][recomb_pair]=[hap_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haploid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haplotypes['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mafs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recombPos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker.rsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mafs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mafs = {}\n",
    "# either use privided MAF or computer MAF\n",
    "if all(vcf_mafs):\n",
    "    print('run this')\n",
    "    for idx, v in enumerate(varnames[item]):\n",
    "        if v not in mafs:\n",
    "            mafs[v] = vcf_mafs[idx]\n",
    "else:\n",
    "    # count founder alleles\n",
    "    for hap in haplotypes[item]:\n",
    "        if not data.tfam.is_founder(hap[1]):\n",
    "            continue\n",
    "        for idxv, v in enumerate(varnames[item]):\n",
    "            if v not in mafs:\n",
    "                # [#alt, #haplotypes]\n",
    "                mafs[v] = [0, 0]\n",
    "            gt = hap[2 + idxv][1] if hap[2 + idxv][0].isupper() else hap[2 + idxv][0]\n",
    "            if not gt == \"?\":\n",
    "                mafs[v][0] += self.gtconv[gt]\n",
    "                mafs[v][1] += 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mafs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcf_mafs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(mafs['V0-176659933'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker.maf_cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_vars = []\n",
    "for v in mafs.keys():\n",
    "    if mafs[v] > maker.maf_cutoff:\n",
    "        exclude_vars.append(v)\n",
    "for i in haplotypes.keys():\n",
    "    haplotypes[i] = listit(haplotypes[i])\n",
    "    for j in range(len(haplotypes[i])):\n",
    "        haplotypes[i][j] = haplotypes[i][j][:2] + \\\n",
    "          [x for idx, x in enumerate(haplotypes[i][j][2:]) if varnames[i][idx] not in exclude_vars]\n",
    "    varnames[i] = [x for x in varnames[i] if x not in exclude_vars]\n",
    "    # handle trivial data\n",
    "    if len(varnames[i]) == 0:\n",
    "        for person in data.families[i]:\n",
    "            data[person] = self.missings\n",
    "        del varnames[i]\n",
    "        del haplotypes[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_exclude_vars=exclude_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_exclude_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recombPos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_vars = []\n",
    "i = '1'\n",
    "for tmp_var in varnames[i]:\n",
    "    if tmp_var not in uniq_vars:\n",
    "             uniq_vars.append(tmp_var)\n",
    "varnames[i] = [x for x in varnames[i] if x not in tmp_exclude_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.genotype_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(varnames):\n",
    "    if not any ([len(varnames[x]) - 1 for x in varnames]):\n",
    "        # all families have only one variant\n",
    "        maker._MarkerMaker__AssignSNVHaplotypes(data, haplotypes, mafs, varnames)\n",
    "    else:\n",
    "        print('run this')\n",
    "        # calculate LD clusters using founder haplotypes\n",
    "        clusters = maker._MarkerMaker__ClusterByLD(data, haplotypes, varnames)\n",
    "        # recoding the genotype of the region\n",
    "        maker._MarkerMaker__CodeHaplotypes(data, haplotypes, mafs, varnames, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def __ClusterByLD(self, data, haplotypes, varnames):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get founder haplotypes\n",
    "founder_haplotypes = []\n",
    "markers = sorted(set(itertools.chain(*varnames.values())), key = lambda x: int(x.split(\"-\")[0][1:]))\n",
    "for item in haplotypes:\n",
    "    for ihap, hap in enumerate(haplotypes[item]):\n",
    "        if not data.tfam.is_founder(hap[1]):\n",
    "            continue\n",
    "        gt = [hap[2 + varnames[item].index(v)] if v in varnames[item] else '?' for v in markers]\n",
    "        founder_haplotypes.append((\"{}-{}\".format(hap[1], ihap % 2), \"\".join([x[1] if x[0].isupper() else x[0] for x in gt])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "founder_haplotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate LD blocks, use r2 measure\n",
    "ld = Align.create(founder_haplotypes).matrixLD(validCharacters=\"12\")[\"r2\"]\n",
    "blocks = []\n",
    "for j in ld:\n",
    "    block = [j]\n",
    "    for k in ld[j]:\n",
    "        if ld[j][k] > maker.r2:\n",
    "            block.append(k)\n",
    "    if len(block) > 1:\n",
    "        blocks.append(block)\n",
    "# get LD clusters\n",
    "clusters = [[markers[idx] for idx in item] for item in list(connected_components(blocks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(connected_components([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(connected_components(blocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def __CodeHaplotypes(self, data, haplotypes, mafs, varnames, clusters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply CHP coding\n",
    "if clusters is not None:\n",
    "    clusters_idx = [[[varnames[item].index(x) for x in y] for y in clusters] for item in haplotypes]\n",
    "else:\n",
    "    clusters_idx = [[[]] for item in haplotypes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker.coder.Execute(haplotypes.values(), [[mafs[v] for v in varnames[item]] for item in haplotypes], clusters_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.superMarkerCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line: [fid, sid, hap1, hap2]\n",
    "for line in maker.coder.GetHaplotypes():\n",
    "    print(line)\n",
    "    if not line[1] in data:\n",
    "        # this sample is not in VCF file. Every variant site should be missing\n",
    "        # they have to be skipped for now\n",
    "        continue\n",
    "    data[line[1]] = (line[2].split(','), line[3].split(','))\n",
    "    if len(data[line[1]][0]) > data.superMarkerCount:\n",
    "        data.superMarkerCount = len(data[line[1]][0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.superMarkerCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get MAF\n",
    "for item in haplotypes:\n",
    "    data.maf[item] = maker.coder.GetAlleleFrequencies(item)\n",
    "    data.maf[item] = tuple(tuple(np.array(v) / np.sum(v)) if np.sum(v) else v\n",
    "                      for v in data.maf[item])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.maf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker._MarkerMaker__FormatHaplotypes(data,recombPos,varnames,uniq_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data['I:1'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # apply CHP coding\n",
    "    for item in data.famvaridx:\n",
    "        if item not in haplotypes and data[data.families[item][0]] != ('0','0'):\n",
    "            # when only wild-type haplotypes are present in a family, still code them instead of ignoring the family\n",
    "            if self.freq_by_fam:\n",
    "                pop=data.freq_by_fam[item]\n",
    "                try:\n",
    "                    varnames[item]=data.total_varnames[pop]\n",
    "                    mafs[item]=data.total_mafs[pop]\n",
    "                except:\n",
    "                    continue\n",
    "            else:\n",
    "                varnames[item]=data.total_varnames['pop']\n",
    "                mafs[item]=data.total_mafs\n",
    "            haplotypes[item]=[]\n",
    "            for person in data.families[item]:\n",
    "                tmp_person=[item, person]\n",
    "                if '00' in data[person]:\n",
    "                    tmp_person+=['?:']*len(varnames[item])\n",
    "                else:\n",
    "                    tmp_person+=['1:']*len(varnames[item])\n",
    "                haplotypes[item].append(tmp_person)\n",
    "                haplotypes[item].append(tmp_person)\n",
    "        elif item in haplotypes:\n",
    "            nonvar_hap_flag=False\n",
    "            #determine if wild-type haplotype is present in a family\n",
    "            for hap in haplotypes[item]:\n",
    "                tmp_genes=[]\n",
    "                for tmpa in hap[2:]:\n",
    "                    if 'A' in tmpa or 'B' in tmpa:\n",
    "                        tmp_genes.append(tmpa[1])\n",
    "                    else:\n",
    "                        tmp_genes.append(tmpa[0])\n",
    "                if set(tmp_genes)==set(['1']):\n",
    "                    #non variant haplotype\n",
    "                    nonvar_hap_flag=True\n",
    "                    break\n",
    "            if not nonvar_hap_flag:\n",
    "                #if family don't have wild-type haplotype, add a fake one to ensure correct coding\n",
    "                var_num=len(varnames[item])\n",
    "                fake_person=[item, 'FAKEPERSON']+['1:']*var_num\n",
    "                haplotypes[item].append(fake_person)\n",
    "            for hidx,hap in enumerate(haplotypes[item]):\n",
    "                if hap[1] in data.missing_persons:\n",
    "                    missing_person=[item,hap[1]]+['?:']*len(varnames[item])\n",
    "                    haplotypes[item][hidx]=missing_person\n",
    "\n",
    "    if not clusters is None:\n",
    "        clusters_idx = [[[varnames[item].index(x) for x in y] for y in clusters] for item in haplotypes]\n",
    "    else:\n",
    "        clusters_idx = [[[]] for item in haplotypes]\n",
    "    if env.debug:\n",
    "        for item in haplotypes:\n",
    "            with env.lock:\n",
    "                print(varnames[item],file=sys.stderr)\n",
    "                print(\"hap{0}\\t{1}\\n\".format(item,haplotypes[item]),file=sys.stderr)\n",
    "    self.coder.Execute(haplotypes.values(), [[mafs[item][v] for v in varnames[item]] for item in haplotypes], clusters_idx)\n",
    "    if env.debug:\n",
    "        with env.lock:\n",
    "            if clusters:\n",
    "                print(\"Family LD clusters: \", clusters_idx, \"\\n\", file = sys.stderr)\n",
    "            self.coder.Print()\n",
    "    # line: [fid, sid, hap1, hap2]\n",
    "    for line in self.coder.GetHaplotypes():\n",
    "        if not line[1] in data:\n",
    "            # this sample is not in VCF file. Every variant site should be missing\n",
    "            # they have to be skipped for now\n",
    "            continue\n",
    "        data[line[1]] = (line[2].split(','), line[4].split(','))\n",
    "        #sub-region count for each sample individual\n",
    "        superMarkerCount=len(data[line[1]][0])\n",
    "        if line[0] not in data.patterns:\n",
    "            data.patterns[line[0]]=[[] for x in range(superMarkerCount)]\n",
    "        for t_Marker in range(superMarkerCount):\n",
    "            t_pat1=line[3].split(',')[t_Marker]\n",
    "            t_pat2=line[5].split(',')[t_Marker]\n",
    "            if t_pat1 not in data.patterns[line[0]][t_Marker]:\n",
    "                data.patterns[line[0]][t_Marker].append(t_pat1)\n",
    "            if t_pat2 not in data.patterns[line[0]][t_Marker]:\n",
    "                data.patterns[line[0]][t_Marker].append(t_pat2)\n",
    "        if len(data[line[1]][0]) > data.superMarkerCount:\n",
    "            data.superMarkerCount = len(data[line[1]][0])\n",
    "    # get MAF\n",
    "    for item in data.famvaridx:\n",
    "        if item not in haplotypes:\n",
    "            for person in data.families[item]:\n",
    "                data[person]=(['0']*data.superMarkerCount,['0']*data.superMarkerCount)\n",
    "    for item in haplotypes:\n",
    "        data.maf[item] = self.coder.GetAlleleFrequencies(item)\n",
    "        if not len(data.maf[item][0]):\n",
    "            continue\n",
    "        data.varnames_by_fam[item]=varnames[item]\n",
    "        wt_maf=0\n",
    "        if self.freq_by_fam:\n",
    "            try:\n",
    "                wt_maf=data.wt_maf[data.freq_by_fam[item]]\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            wt_maf=data.wt_maf['pop']\n",
    "        tmp_data_maf=[]\n",
    "        for v in data.maf[item]:\n",
    "            if len(v)==1:\n",
    "                tmp_data_maf.append((v[0],1-v[0]))\n",
    "            else:\n",
    "                if np.sum(v)<1:\n",
    "                    tmp_ratio=sum(v[1:])/(1-wt_maf)\n",
    "                    tmp_list=[wt_maf]\n",
    "                    if tmp_ratio==0:\n",
    "                        tmp_list.append(1-wt_maf)\n",
    "                    else:\n",
    "                        for tmpv in v[1:]:\n",
    "                            tmp_list.append(tmpv/tmp_ratio)\n",
    "                    tmp_data_maf.append(tuple(tmp_list))\n",
    "                else:\n",
    "                    tmp_data_maf.append(v)\n",
    "        data.maf[item]=tuple(tmp_data_maf)\n",
    "    if env.debug:\n",
    "        with env.lock:\n",
    "            print(\"marker freqs = \", data.maf, \"\\n\", file = sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data.families:\n",
    "    varnames[item], positions, vcf_mafs = data.getFamVariants(item, style = \"map\")\n",
    "    if len(varnames[item]) == 0:\n",
    "        for person in data.families[item]:\n",
    "            data[person] = self.missings\n",
    "        continue\n",
    "    if env.debug:\n",
    "        with env.lock:\n",
    "            sys.stderr.write('\\n'.join(['\\t'.join(x) for x in data.getFamSamples(item)]) + '\\n\\n')\n",
    "    # haplotyping\n",
    "    with env.lock:\n",
    "        if not env.prephased:\n",
    "            with stdoutRedirect(to = env.tmp_log + str(os.getpid()) + '.log'):\n",
    "                haplotypes[item] = self.haplotyper.Execute(data.chrom, varnames[item],\n",
    "                                                       sorted(positions), data.getFamSamples(item))[0]\n",
    "        else:\n",
    "            haplotypes[item] = self.__PedToHaplotype(data.getFamSamples(item))\n",
    "    if len(haplotypes[item]) == 0:\n",
    "        # C++ haplotyping implementation failed\n",
    "        with env.chperror_counter.get_lock():\n",
    "            env.chperror_counter.value += 1\n",
    "    # either use privided MAF or computer MAF\n",
    "    if all(vcf_mafs):\n",
    "        for idx, v in enumerate(varnames[item]):\n",
    "            if v not in mafs:\n",
    "                mafs[v] = vcf_mafs[idx]\n",
    "    else:\n",
    "        # count founder alleles\n",
    "        for hap in haplotypes[item]:\n",
    "            if not data.tfam.is_founder(hap[1]):\n",
    "                continue\n",
    "            for idxv, v in enumerate(varnames[item]):\n",
    "                if v not in mafs:\n",
    "                    # [#alt, #haplotypes]\n",
    "                    mafs[v] = [0, 0]\n",
    "                gt = hap[2 + idxv][1] if hap[2 + idxv][0].isupper() else hap[2 + idxv][0]\n",
    "                if not gt == \"?\":\n",
    "                    mafs[v][0] += self.gtconv[gt]\n",
    "                    mafs[v][1] += 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker.haplotyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = []\n",
    "for _ in range(10):\n",
    "    a = queue.get()\n",
    "    print(a)\n",
    "    tmp.getRegion(a)\n",
    "    tmp.apply(dd)\n",
    "    tmp1.apply(dd)\n",
    "    #tmp2.apply(dd)\n",
    "    if len(dd.variants) != 0:\n",
    "        aa.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = deepcopy(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _MarkerMaker__Haplotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haplotypes = OrderedDict()\n",
    "mafs = {}   ##Per fam per variant\n",
    "uniq_vars = []\n",
    "exclude_vars = []\n",
    "varnames = {}\n",
    "recombPos = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.markers"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def __Haplotype(self, data, haplotypes, mafs, varnames,recombPos,uniq_vars,exclude_vars):\n",
    "'''genetic haplotyping. haplotypes stores per family data'''\n",
    "# FIXME: it is SWIG's (2.0.12) fault not to properly destroy the object \"Pedigree\" in \"Execute()\"\n",
    "# So there is a memory leak here which I tried to partially handle on C++\n",
    "#\n",
    "# Per family haplotyping\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.markers = [\"V{}-{}\".format(idx, item[1]) for idx, item in enumerate(data.variants)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_mafs = {}\n",
    "if tmp1.freq_by_fam:\n",
    "    ## if families are from different populations\n",
    "    ## estimate MAF by different population\n",
    "    fam_to_analyze={}\n",
    "    for fam,pop in data.freq_by_fam.iteritems():\n",
    "        if pop not in fam_to_analyze:\n",
    "            fam_to_analyze[pop]=[fam]\n",
    "        else:\n",
    "            fam_to_analyze[pop].append(fam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tmp1.count:\n",
    "    ## estimate MAF by counting founder alleles\n",
    "    if tmp1.freq_by_fam:\n",
    "        local_count_mafs={}\n",
    "        for pop in fam_to_analyze:\n",
    "            local_count_mafs[pop]=tmp1._MarkerMaker__computefounderfreq(data,fam_to_analyze[pop])\n",
    "    else:\n",
    "        local_count_mafs=tmp1._MarkerMaker__computefounderfreq(data,data.families.keys())\n",
    "        print('run here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_count_mafs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.mle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tmp1.mle:\n",
    "    ## estimate MLE allele frequency using all fam\n",
    "    local_mle_mafs={}\n",
    "    if tmp1.freq_by_fam:\n",
    "        for pop in fam_to_analyze:\n",
    "            local_mle_mafs[pop]={}\n",
    "            markers_to_analyze=[]\n",
    "            pos_all=[]\n",
    "            markers_analyzed={}\n",
    "            if pop not in data.mle_mafs:\n",
    "                data.mle_mafs[pop]={}\n",
    "            else:\n",
    "                for tmpv in data.mle_mafs[pop]:\n",
    "                    markers_analyzed[tmpv.split('-')[-1]]=data.mle_mafs[pop][tmpv]\n",
    "            output_log=env.tmp_log+\"AF_{}_{}.log\".format(pop,tmp1.name)\n",
    "            popidx=tmp1.af_info.index(pop)\n",
    "            variants_in_fams=[]\n",
    "            for item in fam_to_analyze[pop]:\n",
    "                for tmpvar in data.getFamVariants(item):\n",
    "                    if tmpvar not in variants_in_fams:\n",
    "                        variants_in_fams.append(tmpvar)\n",
    "            variants_in_fams=sorted(variants_in_fams, key=lambda x: x[1])\n",
    "            for item in variants_in_fams:\n",
    "                idx=data.variants.index(item)\n",
    "                if item[-1][popidx]==0:\n",
    "                    if str(item[1]) in markers_analyzed.keys():\n",
    "                        #if variant has been analyzed\n",
    "                        vname=\"V{}-{}\".format(idx,item[1])\n",
    "                        local_mle_mafs[pop][vname]=markers_analyzed[str(item[1])]\n",
    "                    else:\n",
    "                        #variant not analyzed before\n",
    "                        markers_to_analyze.append(\"V{}-{}\".format(idx,item[1]))\n",
    "                        pos_all.append(item[1])\n",
    "            tmp_mle_mafs=tmp1._MarkerMaker__getMLEfreq(data, markers_to_analyze, pos_all, fam_to_analyze[pop], tmp1.rsq, output_log)\n",
    "            if len(tmp_mle_mafs) > 0:\n",
    "                for vname,vmaf in tmp_mle_mafs.iteritems():\n",
    "                    data.mle_mafs[pop][vname]=vmaf\n",
    "                    local_mle_mafs[pop][vname]=vmaf\n",
    "    else:\n",
    "        #Homogeneous families\n",
    "        markers_to_analyze=[]\n",
    "        pos_all=[]\n",
    "        markers_analyzed={}\n",
    "        for tmpv in data.mle_mafs:\n",
    "            markers_analyzed[tmpv.split('-')[-1]]=data.mle_mafs[tmpv]\n",
    "        variants_in_fams=[]\n",
    "        for item in data.families.keys():\n",
    "            var_per_fam=[tuple(tmpvar) for tmpvar in data.getFamVariants(item)]\n",
    "            variants_in_fams=list(set(var_per_fam+variants_in_fams))\n",
    "        variants_in_fams=[list(tmpvar) for tmpvar in sorted(variants_in_fams, key=lambda x: x[1])]\n",
    "        for item in variants_in_fams:\n",
    "            idx=data.variants.index(item)\n",
    "            if item[-1]==0 or tmp1.af_info is None:\n",
    "                if str(item[1]) in markers_analyzed.keys():\n",
    "                    #if variant has been analyzed\n",
    "                    vname=\"V{}-{}\".format(idx,item[1])\n",
    "                    local_mle_mafs[vname]=markers_analyzed[str(item[1])]\n",
    "                else:\n",
    "                    #variant not analyzed before\n",
    "                    markers_to_analyze.append(\"V{}-{}\".format(idx,item[1]))\n",
    "                    pos_all.append(item[1])\n",
    "        output_log=env.tmp_log+\"AF_{}.log\".format(tmp1.name)\n",
    "        tmp_mle_mafs=tmp1._MarkerMaker__getMLEfreq(data, markers_to_analyze, pos_all, data.families.keys(), tmp1.rsq, output_log)\n",
    "        if len(tmp_mle_mafs) > 0:\n",
    "            for vname, vmaf in tmp_mle_mafs.iteritems():\n",
    "                data.mle_mafs[vname]=vmaf\n",
    "                local_mle_mafs[vname]=vmaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_mle_mafs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.families.keys()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.getFamVariants(data.families.keys()[1],style=\"map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.famvaridx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.famsampidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnomAD_pop=None\n",
    "for item in data.families:\n",
    "    varnames[item], positions, vcf_mafs = data.getFamVariants(item, style = \"map\")\n",
    "    recombPos[item]={}\n",
    "    var_for_haplotype=[]\n",
    "    positions_for_haplotype=[]\n",
    "    output_sample=[]\n",
    "    if env.debug:\n",
    "        with env.lock:\n",
    "            sys.stderr.write('\\n'+repr(varnames[item])+'\\n')\n",
    "            sys.stderr.write('\\n'.join(['\\t'.join(x) for x in data.getFamSamples(item)]) + '\\n\\n')\n",
    "    # either use privided MAF or compute MAF\n",
    "    if tmp1.freq_by_fam:\n",
    "        mafs[item]={}\n",
    "        tfreq_fam=data.freq_by_fam[item]\n",
    "        for pop in data.gnomAD_estimate.keys():\n",
    "            if pop in tfreq_fam:\n",
    "                gnomAD_pop=pop\n",
    "                break\n",
    "    elif gnomAD_pop is None and data.freq is not None:\n",
    "        for pop in data.gnomAD_estimate.keys():\n",
    "            if pop in data.freq:\n",
    "                gnomAD_pop=pop\n",
    "                break\n",
    "    for idx, v in enumerate(varnames[item]):\n",
    "        tmp_maf_var=0\n",
    "        if tmp1.af_info is None:\n",
    "        #no vcf freq column specified\n",
    "            if v not in tmp_mafs:\n",
    "                if tmp1.mle:\n",
    "                #use MLE freq for all variants\n",
    "                    tmp_mafs[v]=local_mle_mafs[v]\n",
    "                elif tmp1.count:\n",
    "                #estimate MAF based on founder counts if MLE not specified\n",
    "                    tmp_mafs[v]=local_count_mafs[v]\n",
    "                tmp_maf_var=tmp_mafs[v]\n",
    "        elif not tmp1.af_info is None:\n",
    "            #if vcf freq column is specified\n",
    "            #use vcf_mafs if possible\n",
    "            if vcf_mafs[idx]:\n",
    "                tmp_maf_var=vcf_mafs[idx]\n",
    "                if tmp1.freq_by_fam:\n",
    "                    mafs[item][v] = vcf_mafs[idx]\n",
    "                else:\n",
    "                    if v not in tmp_mafs:\n",
    "                        tmp_mafs[v] = vcf_mafs[idx]\n",
    "            else:\n",
    "                #if variants do not have valid vcf_mafs values if specified\n",
    "                if tmp1.freq_by_fam:\n",
    "                    if gnomAD_pop is not None:\n",
    "                        mafs[item][v]=data.gnomAD_estimate[gnomAD_pop]\n",
    "                    elif tmp1.mle:\n",
    "                            mafs[item][v]=local_mle_mafs[data.freq_by_fam[item]][v]\n",
    "                    elif tmp1.count:\n",
    "                            mafs[item][v]=local_count_mafs[data.freq_by_fam[item]][v]\n",
    "                    tmp_maf_var=mafs[item][v]\n",
    "                else:\n",
    "                    if v not in tmp_mafs:\n",
    "                        if gnomAD_pop is not None:\n",
    "                            tmp_mafs[v]=data.gnomAD_estimate[gnomAD_pop]\n",
    "                        elif tmp1.mle:\n",
    "                            tmp_mafs[v]=local_mle_mafs[v]\n",
    "                        elif tmp1.count:\n",
    "                            tmp_mafs[v]=local_count_mafs[v]\n",
    "                    tmp_maf_var=tmp_mafs[v]\n",
    "        if tmp1.rvhaplo:\n",
    "            if tmp_maf_var<=tmp1.maf_cutoff:\n",
    "                var_for_haplotype.append(v)\n",
    "                positions_for_haplotype.append(positions[idx])\n",
    "    if not tmp1.rvhaplo:\n",
    "        var_for_haplotype=varnames[item]\n",
    "        positions_for_haplotype=positions\n",
    "    #collect sample+genotypes\n",
    "    for person in data.tfam.sort_family(item):\n",
    "        output_sample.append([])\n",
    "        last_ele=len(output_sample)-1\n",
    "        output_sample[last_ele] = data.tfam.samples[person][:-1]\n",
    "        if person in data.samples:\n",
    "            for marker in var_for_haplotype:\n",
    "                idx=int(marker.split('-')[0][1:])\n",
    "                output_sample[last_ele].append(data.genotype_all[person][idx])\n",
    "        else:\n",
    "            output_sample[last_ele].extend([\"00\"] * len(var_for_haplotype))\n",
    "    # haplotyping\n",
    "    if len(var_for_haplotype)==0:\n",
    "        varnames.pop(item,None)\n",
    "        #for person in data.families[item]:\n",
    "        #    data[person] = tmp1.missings\n",
    "        continue\n",
    "    for person in output_sample:\n",
    "        if set(person[5:])==set(['00']):\n",
    "            data.missing_persons.append(person[1])\n",
    "    with env.lock:\n",
    "        if not env.prephased:\n",
    "            tmp_log_output=env.tmp_log + str(os.getpid())\n",
    "            with stdoutRedirect(to = tmp_log_output + '.log'):\n",
    "                haplotypes[item] = tmp1.haplotyper.Execute(data.chrom, var_for_haplotype, positions_for_haplotype, output_sample, tmp1.rsq, tmp_log_output)[0]\n",
    "        else:\n",
    "            haplotypes[item] = tmp1.__PedToHaplotype(data.getFamSamples(item))\n",
    "    if len(haplotypes[item]) == 0:\n",
    "        # C++ haplotyping implementation failed\n",
    "        with env.chperror_counter.get_lock():\n",
    "            env.chperror_counter.value += 1\n",
    "    varnames[item]=var_for_haplotype\n",
    "    \n",
    "for item in haplotypes:\n",
    "    for hap_idx,haploid in enumerate(haplotypes[item]):\n",
    "        for vidx,var in enumerate(haploid[2:]):\n",
    "            if not var.endswith(':') and not var.endswith('|') and vidx!=0:\n",
    "                postvar_name=varnames[item][vidx]\n",
    "                prevar_name=varnames[item][vidx-1]\n",
    "                recomb_pair = (prevar_name,postvar_name)\n",
    "                try:\n",
    "                    recombPos[item][recomb_pair].append(hap_idx)\n",
    "                except:\n",
    "                    recombPos[item][recomb_pair]=[hap_idx]\n",
    "#\n",
    "# Compute founder MAFs\n",
    "#\n",
    "if len(tmp_mafs) > 0:\n",
    "    if tmp1.freq_by_fam:\n",
    "        for pop in tmp_mafs:\n",
    "            for v in tmp_mafs[pop]:\n",
    "                if type(tmp_mafs[pop][v]) is list:\n",
    "                    tmp_mafs[pop][v] = tmp_mafs[pop][v][0]/tmp_mafs[pop][v][1] if tmp_mafs[pop][v][1] >0 else 0.0\n",
    "    else:\n",
    "        for v in tmp_mafs:\n",
    "            if type(tmp_mafs[v]) is list:\n",
    "                tmp_mafs[v] = tmp_mafs[v][0]/tmp_mafs[v][1] if tmp_mafs[v][1] > 0 else 0.0\n",
    "## Make mafs consistent in structure regardless of freq_by_fam\n",
    "if tmp1.freq_by_fam:\n",
    "    for item in haplotypes:\n",
    "        popname=data.freq_by_fam[item]\n",
    "        if popname not in tmp_mafs:\n",
    "            continue\n",
    "        if item not in mafs:\n",
    "            mafs[item]=tmp_mafs[popname]\n",
    "        else:\n",
    "            for v in tmp_mafs[popname]:\n",
    "                if v not in mafs[item]:\n",
    "                    mafs[item][v]=tmp_mafs[popname][v]\n",
    "else:\n",
    "    for item in haplotypes:\n",
    "        mafs[item]=tmp_mafs\n",
    "if env.debug:\n",
    "    with env.lock:\n",
    "        print(\"variant mafs = \", mafs, \"\\n\", file = sys.stderr)\n",
    "##\n",
    "#\n",
    "# Drop some variants if maf is greater than given threshold\n",
    "#\n",
    "if not tmp1.maf_cutoff is None or tmp1.single_markers:\n",
    "    if tmp1.freq_by_fam:\n",
    "        exclude_vars=[[] for x in range(len(data.freq))]\n",
    "    for i in haplotypes.keys():\n",
    "        if tmp1.freq_by_fam:\n",
    "            pop_idx=data.freq.index(data.freq_by_fam[i])\n",
    "            tmp_exclude_vars=exclude_vars[pop_idx]\n",
    "        else:\n",
    "            tmp_exclude_vars=exclude_vars\n",
    "        for v in mafs[i].keys():\n",
    "            if not tmp1.maf_cutoff is None:\n",
    "                if mafs[i][v] > tmp1.maf_cutoff and v not in tmp_exclude_vars or v.split('-')[-1] not in data.include_vars:\n",
    "                    tmp_exclude_vars.append(v)\n",
    "            if tmp1.single_markers:\n",
    "                if v.split('-')[-1] not in data.include_vars:\n",
    "                    tmp_exclude_vars.append(v)\n",
    "        haplotypes[i] = listit(haplotypes[i])\n",
    "        tmp_remain_vars=[x for x in varnames[i] if x not in tmp_exclude_vars]\n",
    "        recomb_remain_vars=[]\n",
    "        if len(tmp_remain_vars) == 0:\n",
    "            recombPos[i]={}\n",
    "        else:\n",
    "            if len(recombPos[i]) > 0:\n",
    "                #extend recombination signal to neighbouring RVs\n",
    "                #if the original variant is to be excluded\n",
    "                #Only allow a maximum of one recombination event between one pair of consecutive markers\n",
    "                for pair in recombPos[i].keys():\n",
    "                    if pair[1] not in tmp_exclude_vars:\n",
    "                        if tmp_remain_vars.index(pair[1])!=0 and pair[1] not in recomb_remain_vars:\n",
    "                            recomb_remain_vars.append(pair[1])\n",
    "                        else:\n",
    "                            del recombPos[i][pair]\n",
    "                    else:\n",
    "                        if varnames[i].index(pair[1]) > varnames[i].index(tmp_remain_vars[-1]):\n",
    "                            #last variant\n",
    "                            del recombPos[i][pair]\n",
    "                            continue\n",
    "                        for tmp_idx in range(varnames[i].index(pair[1])+1,len(varnames[i])):\n",
    "                            if varnames[i][tmp_idx] not in tmp_exclude_vars:\n",
    "                                if tmp_remain_vars.index(varnames[i][tmp_idx])==0:\n",
    "                                    #delete recombination pair if the recombination was marked to the first remaining variant\n",
    "                                    del recombPos[i][pair]\n",
    "                                    break\n",
    "                                for tmp_hap in recombPos[i][pair]:\n",
    "                                    tmp_var=haplotypes[i][tmp_hap][tmp_idx+2]\n",
    "                                    if tmp_var.endswith(':') or tmp_var.endswith('|'):\n",
    "                                        haplotypes[i][tmp_hap][tmp_idx+2]=tmp_var[:-1]+'/'\n",
    "                                if varnames[i][tmp_idx] not in recomb_remain_vars:\n",
    "                                    recomb_remain_vars.append(varnames[i][tmp_idx])\n",
    "                                else:\n",
    "                                    del recombPos[i][pair]\n",
    "                                break\n",
    "        for j in range(len(haplotypes[i])):\n",
    "            haplotypes[i][j] = haplotypes[i][j][:2] + \\\n",
    "              [x for idx, x in enumerate(haplotypes[i][j][2:]) if varnames[i][idx] not in tmp_exclude_vars]\n",
    "        for tmp_var in varnames[i]:\n",
    "            if tmp_var not in uniq_vars:\n",
    "                     uniq_vars.append(tmp_var)\n",
    "        varnames[i] = [x for x in varnames[i] if x not in tmp_exclude_vars]\n",
    "        # handle trivial data\n",
    "        if len(varnames[i]) == 0:\n",
    "            del varnames[i]\n",
    "            del haplotypes[i]\n",
    "        if len(recombPos[i].keys())>tmp1.recomb_max:\n",
    "            #treat as missing if recombination events occurred more than speicified times\n",
    "            recombPos[i]={}\n",
    "            for person in data.families[i]:\n",
    "                data[person] = tmp1.missings\n",
    "            del varnames[i]\n",
    "            del haplotypes[i]\n",
    "    # count how many variants are removed\n",
    "    with env.commonvar_counter.get_lock():\n",
    "        if tmp1.freq_by_fam:\n",
    "            tmp_ex_vars=[tmp_var for tmp_vars in exclude_vars for tmp_var in tmp_vars]\n",
    "            env.commonvar_counter.value += len(set(tmp_ex_vars))\n",
    "        else:\n",
    "            env.commonvar_counter.value += len(exclude_vars)\n",
    "    # get total observed variants\n",
    "    if tmp1.freq_by_fam:\n",
    "        for item in varnames:\n",
    "            pop=data.freq_by_fam[item]\n",
    "            if pop not in data.total_mafs:\n",
    "                data.total_mafs[pop]={}\n",
    "                data.total_varnames[pop]=[]\n",
    "            for v in varnames[item]:\n",
    "                if v not in data.total_mafs[pop]:\n",
    "                    data.total_varnames[pop].append(v)\n",
    "                    data.total_mafs[pop][v]=mafs[item][v]\n",
    "        for pop in data.total_varnames:\n",
    "            data.total_varnames[pop]=sorted(data.total_varnames[pop], key=lambda x: int(x.split(\"-\")[0][1:]))\n",
    "            data.wt_maf[pop]=1.0\n",
    "            for v,tmaf in data.total_mafs[pop].iteritems():\n",
    "                data.wt_maf[pop]*=(1-tmaf)\n",
    "    else:\n",
    "        data.total_varnames['pop']=[]\n",
    "        for item in varnames:\n",
    "            for v in varnames[item]:\n",
    "                if v not in data.total_mafs:\n",
    "                    data.total_varnames['pop'].append(v)\n",
    "                    data.total_mafs[v]=mafs[item][v]\n",
    "        data.wt_maf['pop']=1.0\n",
    "        for v,tmaf in data.total_mafs.iteritems():\n",
    "            data.wt_maf['pop']*=(1-tmaf)\n",
    "        data.total_varnames['pop']=sorted(data.total_varnames['pop'], key=lambda x: int(x.split(\"-\")[0][1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.total_varnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??cstatgen.HaplotypingEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.wt_maf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = []\n",
    "for _ in range(1000):\n",
    "    a = queue.get()\n",
    "    tmp.getRegion(a)\n",
    "    tmp.apply(dd)\n",
    "    if len(dd.variants) != 0:\n",
    "        aa.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.getRegion(aa[0])\n",
    "dd = deepcopy(data)\n",
    "tmp.apply(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.apply(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1._MarkerMaker__Haplotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.freq_by_fam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.rvhaplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.apply(dd)\n",
    "tmp2.apply(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.freq_by_fam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.prephased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.tmp_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haplotypes = OrderedDict()\n",
    "mafs = {}   ##Per fam per variant\n",
    "uniq_vars = []\n",
    "exclude_vars = []\n",
    "varnames = {}\n",
    "recombPos = {}\n",
    "tmp1._MarkerMaker__Haplotype(dd, haplotypes, mafs, varnames,recombPos,uniq_vars,exclude_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.recomb_perfam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.apply(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp2.apply(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.include_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.getRegion(aa[0])\n",
    "tmp1.apply(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.debug =True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.chrom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??EncoderWorker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.total_counter.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in jobs:\n",
    "    j.start()\n",
    "for j in jobs:\n",
    "    j.join()\n",
    "faulthandler.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    try:\n",
    "        faulthandler.enable(file=open(env.tmp_log + '.SEGV', 'w'))\n",
    "        for i in regions:\n",
    "            if isinstance(queue, list):\n",
    "                queue.append(i)\n",
    "            else:\n",
    "                queue.put(i)\n",
    "        freq_by_fam_flag = False\n",
    "        if not args.freq_by_fam is None:\n",
    "            print('haha')\n",
    "            freq_by_fam_flag = True\n",
    "            with open(args.freq_by_fam) as freq_fh:\n",
    "                for freq_line in freq_fh:\n",
    "                    tmp_eles=freq_line.split()   #Fam and Population\n",
    "                    data.freq_by_fam[tmp_eles[0]]=tmp_eles[1]\n",
    "            data.freq=sorted(list(set(data.freq_by_fam.values())))\n",
    "        else:\n",
    "            data.freq=args.freq\n",
    "        jobs = [EncoderWorker(\n",
    "            queue, len(regions), deepcopy(data),\n",
    "            RegionExtractor(args.vcf, chr_prefix = args.chr_prefix, allele_freq_info = data.freq, include_vars_file=args.include_vars),\n",
    "            MarkerMaker(args.bin, maf_cutoff = args.maf_cutoff,single_markers=args.single_markers,recomb_max=args.recomb_max,af_info=data.freq,freq_by_fam=freq_by_fam_flag,rsq=args.rsq,mle=args.mle, rvhaplo=args.rvhaplo, recomb_perfam=not args.recomb_cross_fam),\n",
    "            LinkageWriter(len(samples_not_vcf))\n",
    "            ) for i in range(env.jobs)]\n",
    "        for j in jobs:\n",
    "            j.start()\n",
    "        for j in jobs:\n",
    "            j.join()\n",
    "        faulthandler.disable()\n",
    "    except KeyboardInterrupt:\n",
    "        # FIXME: need to properly close all jobs\n",
    "        raise ValueError(\"Use 'killall {}' to properly terminate all processes!\".format(env.prog))\n",
    "    else:\n",
    "        env.log('{:,d} units (from {:,d} variants) processed; '\\\n",
    "            '{:,d} Mendelian inconsistencies and {:,d} recombination events handled\\n'.\\\n",
    "            format(env.success_counter.value,\n",
    "                   env.variants_counter.value,\n",
    "                   env.mendelerror_counter.value,\n",
    "                   env.recomb_counter.value), flush = True)\n",
    "        if env.triallelic_counter.value:\n",
    "            env.log('{:,d} tri-allelic loci were ignored'.format(env.triallelic_counter.value))\n",
    "        if env.commonvar_counter.value:\n",
    "            env.log('{:,d} variants ignored due to having MAF > {} and other specified constraints'.\\\n",
    "                    format(env.commonvar_counter.value, args.maf_cutoff))\n",
    "        if env.null_counter.value:\n",
    "            env.log('{:,d} units ignored due to absence in VCF file'.format(env.null_counter.value))\n",
    "        if env.trivial_counter.value:\n",
    "            env.log('{:,d} units ignored due to absence of variation in samples'.format(env.trivial_counter.value))\n",
    "        fatal_errors = 0\n",
    "        try:\n",
    "            # Error msg from C++ extension\n",
    "            os.system(\"cat {}/*.* > {}\".format(env.tmp_dir, env.tmp_log))\n",
    "            fatal_errors = wordCount(env.tmp_log)['fatal']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        if env.chperror_counter.value:\n",
    "            env.error(\"{:,d} regional markers failed to be generated due to haplotyping failures!\".\\\n",
    "                      format(env.chperror_counter.value))\n",
    "        if fatal_errors:\n",
    "            env.error(\"{:,d} or more regional markers failed to be generated due to runtime errors!\".\\\n",
    "                      format(fatal_errors))\n",
    "        env.log('Archiving regional marker data to directory [{}]'.format(env.cache_dir))\n",
    "        cache.write(arcroot = 'CACHE', source_dir = env.tmp_cache)\n",
    "env.jobs = args.jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: write to PLINK or mega2 format\n",
    "tpeds = [os.path.join(env.tmp_cache, item) for item in os.listdir(env.tmp_cache) if item.startswith(env.output) and item.endswith('.tped')]\n",
    "for fmt in args.format:\n",
    "    cache.setID(fmt)\n",
    "    if not args.vanilla and cache.check():\n",
    "        env.log('Loading {} data from archive ...'.format(fmt.upper()))\n",
    "        cache.load(target_dir = env.tmp_dir, names = [fmt.upper()])\n",
    "    else:\n",
    "        env.log('{:,d} units will be converted to {} format'.format(env.success_counter.value, fmt.upper()))\n",
    "        env.format_counter.value = 0\n",
    "        format(tpeds, os.path.join(env.tmp_cache, \"{}.tfam\".format(env.output)),\n",
    "               args.prevalence, args.wild_pen, args.muta_pen, fmt,\n",
    "               args.inherit_mode, args.theta_max, args.theta_inc)\n",
    "        env.log('{:,d} units successfully converted to {} format\\n'.\\\n",
    "                format(env.format_counter.value, fmt.upper()), flush = True)\n",
    "        if env.skipped_counter.value:\n",
    "            # FIXME: perhaps we need to rephrase this message?\n",
    "            env.log('{} region - family pairs skipped'.\\\n",
    "                    format(env.skipped_counter.value))\n",
    "        env.log('Archiving {} format to directory [{}]'.format(fmt.upper(), env.cache_dir))\n",
    "        cache.write(arcroot = fmt.upper(),\n",
    "                    source_dir = os.path.join(env.tmp_dir, fmt.upper()), mode = 'a')\n",
    "mkpath(env.outdir)\n",
    "if args.run_linkage:\n",
    "    cache.setID('analysis')\n",
    "    if not args.vanilla and cache.check():\n",
    "        env.log('Loading linkage analysis result from archive ...'.format(fmt.upper()))\n",
    "        cache.load(target_dir = env.output, names = ['heatmap'])\n",
    "    else:\n",
    "        env.log('Running linkage analysis ...'.format(fmt.upper()))\n",
    "        run_linkage(args.blueprint, args.theta_inc, args.theta_max, args.output_limit)\n",
    "        env.log('Linkage analysis succesfully performed for {:,d} units\\n'.\\\n",
    "                format(env.run_counter.value, fmt.upper()), flush = True)\n",
    "        if env.makeped_counter.value:\n",
    "            env.log('{} \"makeped\" runtime errors occurred'.format(env.makeped_counter.value))\n",
    "        if env.pedcheck_counter.value:\n",
    "            env.log('{} \"pedcheck\" runtime errors occurred'.format(env.pedcheck_counter.value))\n",
    "        if env.unknown_counter.value:\n",
    "            env.log('{} \"unknown\" runtime errors occurred'.format(env.unknown_counter.value))\n",
    "        if env.mlink_counter.value:\n",
    "            env.log('{} \"mlink\" runtime errors occurred'.format(env.mlink_counter.value))\n",
    "        cache.write(arcroot = 'heatmap', source_dir = os.path.join(env.output, 'heatmap'), mode = 'a')\n",
    "    html(args.theta_inc, args.theta_max, args.output_limit)\n",
    "else:\n",
    "    env.log('Saving data to [{}]'.format(os.path.abspath(env.output)))\n",
    "    cache.load(target_dir = env.output, names = [fmt.upper() for fmt in args.format])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
